{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq_3t_tluNmU"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TQVMUZkQHlm2"
   },
   "source": [
    "## 1. Classifying Digits\n",
    "In this part we will test digits classification on the MNIST dataset, using Bernoulli Naive Bayes (a generative model).\n",
    "\n",
    "The MNIST dataset contains 28x28 grayscale images of handwritten digits between 0 and 9 (10 classes). For mathmatical analysis clarity, and for matching expected API, flatten each image to create a 1D array with 784 elements.\n",
    "\n",
    "* The fuck is EXPECTED API?!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjwjk6pzLE-y"
   },
   "source": [
    "### Loading the MNIST dataset\n",
    "Load the MNIST data set. The digits dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. Use \n",
    ">```\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "```\n",
    "\n",
    "Plot a single sample of each digit as the original image, so you get a feeling how the data looks like.\n",
    "\n",
    "Finally, divide your data into train and test sets, using 1/7 of the data for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTGjjSKaFfE6"
   },
   "source": [
    "### Bernoulli Naive Bayes\n",
    "If we know how the digits are generated, then we know how to classify them (simply choose the digit class which will maximize the posterior probability) --- but which model should we use for describing the digits generation?\n",
    "\n",
    "In this part we will try a very simplified model of digits creation (which is obviously not the same as the \"real\" model), using a Naive Bayes over an underlying Bernoulli distribution --- that is, we will assume that given a digit class, the pixels of the images are the result of independent coin flips, each with its own \"head\" probability.\n",
    "\n",
    "Note that since we assume each pixl is either 0 (black) or 1 (white), we will need to adjust (preprocess) our data accrodingly (see below).\n",
    "\n",
    "So, the model is stated as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Domain} && x \\in \\{0,1\\}^{784} \\\\\n",
    "\\text{Prior} && \\pi_j = \\Pr(y=j) \\\\\n",
    "\\text{Likelihood} && P_j(x) = \\Pr(x | y=j) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where for each $i\\in 0\\ldots 784$ it holds that\n",
    "$$\n",
    "P_{ji}(x_i) = \\Pr(x_i | y=j) =\n",
    "\\begin{cases}\n",
    "p_{ji} & \\text{if } x_i=1 \\\\\n",
    "1-p_{ji} & \\text{if } x_i=0 \\\\\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNjhD3IpL5bC"
   },
   "source": [
    "#### Question 1\n",
    "Write the classification rule based on this Naive Bayes model. \n",
    "How would you esitmate each of the parameters of the model based on the trainning data? \n",
    "\n",
    "\n",
    "**Bonus:** Think of edge cases which may effect your estimator in an undesired way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pN1prGcMqwZ"
   },
   "source": [
    "#### Answer 1\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOnkgDIXTMCQ"
   },
   "source": [
    "#### Question 2\n",
    "Run a Naive Bayes classifier on the training data and apply predictions on the test data. Use the [sklearn.naive_bayes.BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) implementation (see the [source code for sklearn.naive_bayes](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py) for details).\n",
    "\n",
    "Remember we need to preprocess the data in this case such that each pixel would become either black (0) or white (1). For this, use the `binarize` parameter of the implementation. Set this value to $0$ (this is the default), which in this case would mean every pixel with non-zero value will be set to 1.\n",
    "\n",
    "1. Plot the mean image of each class (estimated $\\hat{p}_{ji}$) and generate one sample of each class (remember, you can do this since this is a generative model). You will need to access the `feature_log_prob_` attribute of the trained model.\n",
    "\n",
    "2. Plot the confusion matrix of your classifier, as claculated on the test data (it is recommended to use [sklearn.metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)). Calculate the total accuracy (fraction of correctly classified images), and summarize the results in your own words.\n",
    "\n",
    "3. Think of a way you can find the optimal threshold of the binarization part. **There is no need to actually perform this task --- just describe what you would have done.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKdsdegDWaO_"
   },
   "source": [
    "#### Answer 2\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XalqjRWXWS-Y"
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKSgnectrTJ1"
   },
   "source": [
    "## 2. Classifing Text Documents using Multinomial Naive Bayes\n",
    "In this exercise you will classify the \"20 newsgroups\" data set using your own naive bayes classifier and compare to the scikit learn built in version.\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon messages posted before and after a specific date.\n",
    "\n",
    "\n",
    "* Load the **train** data using `from sklearn.datasets import fetch_20newsgroups`. remove headers, footers and quotes (see documentation)\n",
    "* Use `sklearn.feature_extraction.text import CountVectorizer` to count words (stop_words='english')\n",
    "* Write a class `NaiveBayes(BaseEstimator, ClassifierMixin)` and implement its `fit`, `predict` and `predict_proba` methods.\n",
    "* use `sklearn.pipeline.make_pipeline` to chain the vectroizer and model.\n",
    "* note: limit the vocuabolary size if you suffer memory issues\n",
    "* compare the accuracy over the **test** data. You can use `accuracy_score, classification_report`\n",
    "* compare to the built in `sklearn.naive_bayes.MultinomialNB`. If there are differences try to think why\n",
    "* plot the learning curve - is the model in the bias or variance regime (you can use the built in model for doing the analysis)\n",
    "* optimize performance in respect to vectorizer hyper parameters (e.g. max_features, max_df etc.).\n",
    "\n",
    "### Optional: Model interpretability\n",
    "Find the most important features for a **specific** decision of a NB classifier.\n",
    "Because the model has learned the prior $p(x_i|c)$ during the training, the contribution of an individual feature value can be easily measured by the posterior, $p(c|x_i)=p(c)p(x_i|c)/p(x_i)$\n",
    "Implement a function which gets a scikit-learn NB model as input and returns $P(c|x_i)$:\n",
    "\n",
    "`def calc_p_c_given_xi(model)`\n",
    "\n",
    "Hint: Use the following model properties:\n",
    "\n",
    "* `model.class_log_prior_`\n",
    "* `model.feature_log_prob_`\n",
    "\n",
    "Note: remember these are logs and you need to use np.exp and normalize to get $P(c|x_i)$ \n",
    "Another hint: use numpy built-in broadcasting property.\n",
    "\n",
    "* Use the interpretation to examine errors of the classifier where $\\hat{c}\\ne c$. Which top words support the correct class and which support the wrong class? You can use the `print_txt` below to color words. \n",
    "\n",
    "Bonus: How can you correct the analyzed error? \n",
    "\n",
    "To read more about model interpretation, see the blogpost below and my tutorial:\n",
    "* https://lilianweng.github.io/lil-log/2017/08/01/how-to-explain-the-prediction-of-a-machine-learning-model.html\n",
    "* https://github.com/chanansh/right_but_why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WrTOhkHV9msW",
    "outputId": "c69cb41c-5ed6-4e43-ef3f-6ccaa4980c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This \u001b[42;37mword\u001b[0m support the first class but this the \u001b[41;37mother\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def print_txt(txt, hot, cold):\n",
    "  \"\"\"\n",
    "  print the text, coloring hot and cold words with colors\n",
    "  \"\"\"\n",
    "  cold_color='\\x1b[41;37m{}\\x1b[0m'\n",
    "  hot_color='\\x1b[42;37m{}\\x1b[0m'\n",
    "  def color(token):\n",
    "    lower = str(token).lower()\n",
    "    lower = lower.replace('\\t','').replace('\\n','')\n",
    "    lower = lower.translate(string.punctuation)\n",
    "    if (lower in hot) and (lower in cold):\n",
    "      return mid_color.format(token)\n",
    "    elif lower in hot:\n",
    "      return hot_color.format(token)\n",
    "    elif lower in cold:\n",
    "      return cold_color.format(token)\n",
    "    else:\n",
    "      return token\n",
    "  colored_txt = \" \".join([color(token) for token in txt.split(' ')])\n",
    "  print(colored_txt)\n",
    "print_txt('This word support the first class but this the other', ['word'],['other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEbVc4Ixxe6L"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "  def fit(self, x, y):\n",
    "    pass\n",
    "  def predict_log_proba(self, x):\n",
    "    pass  \n",
    "  def predict(self, x):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NB exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d04047d2253f287091e9caf71fa3f6c1ebc788e61497cfe430ec057946d30f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
