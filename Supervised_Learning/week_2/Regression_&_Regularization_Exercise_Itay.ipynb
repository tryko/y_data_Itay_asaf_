{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chEqqJbLzFew"
   },
   "source": [
    "# Yandex Data Science School\n",
    "## Linear Regression & Regularization Exercise.\n",
    "\n",
    "\n",
    "## Outline\n",
    "In this exercise you will learn the following topics:\n",
    "\n",
    "1. Refresher on how linear regression is solved in batch and in Gradient Descent \n",
    "2. Implementation of Ridge Regression\n",
    "3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git Exercise\n",
    "In this exercise you will also experience working with github.\n",
    "\n",
    "You might need to install local python enviroment.\n",
    "Installation Instruction for ex2 - working on a local python environment:\n",
    "https://docs.google.com/document/d/1G0rBo36ff_9JzKy0EkCalK4m_ThNUuJ2bRz463EHK9I\n",
    "\n",
    "## please add the github link of your work below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: https://github.com/username/exercise_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR9UFmk2greT"
   },
   "source": [
    "## Refresher on Ordinary Least Square (OLS) aka Linear Regeression\n",
    "\n",
    "### Lecture Note\n",
    "\n",
    "In Matrix notation, the matrix $X$ is of dimensions $n \\times p$ where each row is an example and each column is a feature dimension. \n",
    "\n",
    "Similarily, $y$ is of dimension $n \\times 1$ and $w$ is of dimensions $p \\times 1$.\n",
    "\n",
    "The model is $\\hat{y}=X\\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.\n",
    "\n",
    "Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS): \n",
    "\n",
    "$L_{RSS}=\\frac{1}{N}\\left\\Vert Xw-y \\right\\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)\n",
    "\n",
    "To find the optimal $w$ one needs to derive the loss with respect to $w$.\n",
    "\n",
    "$\\frac{\\partial{L_{RSS}}}{\\partial{w}}=\\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )\n",
    "\n",
    "Thus, the gardient descent solution is $w'=w-\\alpha \\frac{2}{N}X^T(Xw-y)$.\n",
    "\n",
    "Solving $\\frac{\\partial{L_{RSS}}}{\\partial{w}}=0$ for $w$ one can also get analytical solution:\n",
    "\n",
    "$w_{OLS}=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.\n",
    "\n",
    "See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA3MEKz80vdy"
   },
   "source": [
    "## Exercise 1 - Ordinary Least Square\n",
    "* Get the boston housing dataset by using the scikit-learn package. hint: [load_boston](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)\n",
    "\n",
    "* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)\n",
    "\n",
    "* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
    "\n",
    "* Fit the model. What is the training MSE?\n",
    "\n",
    "* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\\hat{Y}_{OLS}$\n",
    "\n",
    "* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
    "\n",
    "* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html). \n",
    "\n",
    "* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.\n",
    "\n",
    "* The following parameters are optional (not mandatory to use):\n",
    "    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.\n",
    "    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)\n",
    "    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZuSS8LhcfZdn"
   },
   "outputs": [],
   "source": [
    "# * write a model `Ols` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score`.? hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
    "\n",
    "class Ols(object):\n",
    "  def __init__(self):\n",
    "    self.w = None\n",
    "    \n",
    "  @staticmethod\n",
    "  def pad(X):\n",
    "    pass\n",
    "  \n",
    "  def fit(self, X, Y):\n",
    "    #remeber pad with 1 before fitting\n",
    "    pass\n",
    "  \n",
    "  def _fit(self, X, Y):\n",
    "    # optional to use this\n",
    "    pass\n",
    "  \n",
    "  def predict(self, X):\n",
    "    #return wx\n",
    "    pass\n",
    "  \n",
    "  def _predict(self, X):\n",
    "    # optional to use this\n",
    "    pass\n",
    "    \n",
    "  def score(self, X, Y):\n",
    "    #return MSE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a new class OlsGd which solves the problem using gradinet descent. \n",
    "# The class should get as a parameter the learning rate and number of iteration. \n",
    "# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.\n",
    "# What is the effect of learning rate? \n",
    "# How would you find number of iteration automatically? \n",
    "# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.\n",
    "class Normalizer():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def fit(self, X):\n",
    "    pass\n",
    "\n",
    "  def predict(self, X):\n",
    "    #apply normalization\n",
    "    pass\n",
    "    \n",
    "class OlsGd(Ols):\n",
    "  \n",
    "  def __init__(self, learning_rate=.05, \n",
    "               num_iteration=1000, \n",
    "               normalize=True,\n",
    "               early_stop=True,\n",
    "               verbose=True):\n",
    "    \n",
    "    super(OlsGd, self).__init__()\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_iteration = num_iteration\n",
    "    self.early_stop = early_stop\n",
    "    self.normalize = normalize\n",
    "    self.normalizer = Normalizer()    \n",
    "    self.verbose = verbose\n",
    "    \n",
    "  def _fit(self, X, Y, reset=True, track_loss=True):\n",
    "    #remeber to normalize the data before starting\n",
    "    pass\n",
    "        \n",
    "  def _predict(self, X):\n",
    "    #remeber to normalize the data before starting\n",
    "    pass\n",
    "      \n",
    "  def _step(self, X, Y):\n",
    "    # use w update for gradient descent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVfnXvZFi98"
   },
   "source": [
    "## Exercise 2 - Ridge Linear Regression\n",
    "\n",
    "Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:\n",
    "\n",
    "$L(y,\\hat{y})=\\sum_{i=1}^{i=N}{(y^{(i)}-\\hat{y}^{(i)})^2} + \\lambda \\left\\Vert w \\right\\Vert_2^2$\n",
    "\n",
    "where $y^{(i)}$ is the **true** value and $\\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples\n",
    "\n",
    "* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
    "* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression (do not use the random noise analogy but use the analytical derivation). Either add a parameter, or use inheritance.\n",
    "* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeLs(Ols):\n",
    "  def __init__(self, ridge_lambda, *wargs, **kwargs):\n",
    "    super(RidgeLs,self).__init__(*wargs, **kwargs)\n",
    "    self.ridge_lambda = ridge_lambda\n",
    "    \n",
    "  def _fit(self, X, Y):\n",
    "    #Closed form of ridge regression\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use scikitlearn implementation for OLS, Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression & Regularization - Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d04047d2253f287091e9caf71fa3f6c1ebc788e61497cfe430ec057946d30f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
