{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHjry4MFiQZ6"
      },
      "source": [
        "# Support Vector Machine Exercise\n",
        "In this exercise you will learn about:\n",
        "1. Implementing SVM from scratch using a sub-gradient method called Pegasos (2011)\n",
        "2. The effect of imbalance and non-seperable classes on the SVM solution.\n",
        "3. Pratical SVM in scikit-learn on a simple example including hyper parameter optimization wrapper class to find optimal regularization, loss and multiclass technique.\n",
        "4. Optional reading material on one class, new probability interpretation of SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8w86nqjxQe"
      },
      "source": [
        "## 1. Implementing the PEGASOS SVM\n",
        "We will implement the simplest SVM implementation. \n",
        "It is based on a paper by Shalev-Shwartz et al (see reading material below).\n",
        "The essense of the algorithm is copied below from the paper:\n",
        "![alt text](https://snag.gy/Gpi1Mk.jpg)\n",
        "\n",
        "Here is a little more deailed explanation (it's recommended to read the full paper).\n",
        "![alt_text](https://snag.gy/koA0ue.jpg)\n",
        "\n",
        "1. Implement a class *PegasosSVM* which has parameter $\\lambda$ and $T$ and methods *fit* and *predict* and *decision_function* where the latter is the distance from the plane (aka model's output score).\n",
        "  * **although the above**, here you are required to pad the samples with 1, and not use bias (the separating hyperplane must go through the origin).\n",
        "  * don't forget labels should be {-1,1} - change them if it is {0,1}\n",
        "  * don't forget to normalize your features. You can use [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to create a pipeline which first standartize features and then learn.\n",
        "  * decision function is simply the Euclidean distance (with a sign that designates from which side of the decision plane the point is located) from the decision plane to each sample. So, if you query for some point, you will get a positive number if the point is above the plain and negative if it is below it. The value will give you the distance which you can use for a scoring, for example, after you normalize it. If a point is very near to the decision plane it will have a lower score than a point that is farther. Also, if your point is located inside the margins of the model, you may want to treat the score as a probability for assigning the point to one class or another.\n",
        "\n",
        "2. test your class on the breast cancer database [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), and compare your results to the Native Scikit-learn implementation [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
        "  * you can use [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
        "\n",
        "3. Analyze the effect of the hyper parameter $\\lambda$ on your training and test error. \n",
        "  * you can use scikit-learn's `validation_curve`\n",
        "\n",
        "4. Analyze the learning curve (performance as function of training size)\n",
        "  * you can use scikit-learn's `learning_curve`\n",
        "\n",
        "5. *Bonus: Mini-batches* . In the paper, the authors summed the loss over several samples before updating. Extend your class to support mini-batches and analyze the perfomance effect if exists. See more details below\n",
        "\n",
        "![](https://i.ibb.co/0BGwVz7/1.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owvj20y33dqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVj66a0uEwoI"
      },
      "source": [
        "## 2. The importance of class weighting in SVM\n",
        "When classes are imbalance and not seperable, SVM might result in non intuitive solution.\n",
        "\n",
        "To see this we created for you a function `make_data` which generates a two dimensional dataset with `n_samples=1000` where a fraction `imbalance=0.1` is labeled `0` and the rest `1`. The distribution of $x$ given $y=0$ or $y=1$ is gaussian with std 0.5 or 2, respectively and that the centers are at $x_1=0$ and $x_2=-d/2$ and $x_2=d/2$, respectively. In other words,  \n",
        "$$\n",
        "p(x|y) = \\mathcal{N}(\\mu=(¬±d/2,0),\\,\\sigma=1/2+3/2\\times y)\n",
        "$$\n",
        "and \n",
        "$$\n",
        "p(y=0) = 0.1\n",
        "$$. \n",
        "\n",
        "TODO:\n",
        "1. For `d=10` and `d=2` plot the scatter plot of the data. Where would you think the SVM hyperplane will lie?\n",
        "\n",
        "2. Now, for each of these d, draw on top of the scatter the seperation hyperplane of the built-in LinearSVC in scikit-learn.\n",
        "\n",
        "To plot the hyperplane, note that the fitted model has the `coef_` and `intercept_` properties.\n",
        "Add also the `accuracy` and the `balanced_accuracy` metrics to the plot title.\n",
        "\n",
        "3. repeat the experiment but now set the `class_weight` to be `balanced`. \n",
        "The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
        "\n",
        "Explain what was the problem and how changing class weight solved it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8gD9zjYdVq"
      },
      "source": [
        "## Answer #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MdH46gcYdVq"
      },
      "outputs": [],
      "source": [
        "# a function make_data which generates a two dimensional dataset with n_samples=1000 where a fraction imbalance=0.1 is\n",
        "# labeled 0 and the rest 1. The distribution of  ùë•  given  ùë¶=0  or  ùë¶=1  is gaussian with std 0.5 or 2, respectively and that\n",
        "# the centers are at  ùë•1=0  and  ùë•2=‚àíùëë/2  and  ùë•2=ùëë/2 ,\n",
        "\n",
        "def make_data(d, imbalance=0.1):\n",
        "    n_samples=1000\n",
        "    X = np.zeros((n_samples, 2))\n",
        "    y = np.random.binomial(1, 1-imbalance, n_samples)\n",
        "    size_0 = np.sum(y==0)\n",
        "    size_1 = np.sum(y==1)\n",
        "    X[y==0,0] = np.random.normal(loc=0, scale=0.5, size=size_0)\n",
        "    X[y==1,0] = np.random.normal(loc=0, scale=2, size=size_1)\n",
        "    X[y==0,1] = np.random.normal(loc=-d/2, scale=0.5, size=size_0)\n",
        "    X[y==1,1] = np.random.normal(loc=d/2, scale=2, size=size_1)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ckBPjME35EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-RXyr-hJQt7"
      },
      "source": [
        "## 3. SVM hyperparameter search\n",
        "Machine learning pipelines in general and models in particular has several hyperparametrs that we currently do not know how to optimize.\n",
        "Instead, researcher use grid search or random search techniques to find the optimal hyper parameters.\n",
        "In this exercise you are requested to compare between default params model and a model which internally optimize for these hyper parameters.\n",
        "Luckily, scikit-learn already has a ready class for doing that called `GridSearchCV`. Use the latter class to check all the combination of the following hyper parameters:\n",
        "* `'multi_class':['ovr', 'crammer_singer']`\n",
        "* `'loss':['hinge', 'squared_hinge']`\n",
        "* `'C': np.logspace(-3,3,10)`\n",
        "\n",
        "Compare the accuracy of the model to a default `LinearSVC` model with no hyper parameter optimization in terms of accuracy.\n",
        "\n",
        "Note: Recall, that hyper parameter search is a learning procedure by itself, thus one should not use test data for the process. \n",
        "\n",
        "To do this you can use the handy `cross_val_score` function.\n",
        "\n",
        "Use the cancer dataset for this question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MY0eEV3YdVs"
      },
      "source": [
        "## Answer #"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBJq4bJz394B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28AoyabLt-d4"
      },
      "source": [
        "## 4. Classifying Checkerboard with SVM (20)\n",
        "In this exercise you will try to choose a kernel for classifying a checkerboard labeled data. \n",
        "\n",
        "* Use the `checkboard` function below to generate\n",
        "your training set (1000 or 2000 samples) and a testing set (as much as you like). As you can see the function generates samples in 2D and label them according to a 3 √ó 3 checkerboard (obviously linear separation will not do the job here). Python‚Äôs scatter in the might come in handy.\n",
        "\n",
        "* Use SVM to find a good linear separation in high dimensional feature space using the polynomial or the Gaussian (RBF) kernel.    You should evaluate for both models the best parameters to use. Explain how do you evalute each model?\n",
        "   - For polynomial kernel, estimate which degree to use\n",
        "   - For RBF, estimate which standard deviation to use\n",
        "   - For both models, also choose which regularization parameter is best to use\n",
        " \n",
        "* For the best model of each kernel, plot a scatter plot of the training patterns (colored by label) and mark the support vectors as well. Which model would you choose to use in this case?\n",
        "\n",
        "* In addition, if you are curious, you can try and think of a way to plot the separating hyper-surface (curve in this case), the margin, and the testing set (to check which points are misclassified)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxmQ7J5Lt-d4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def checkerboard(n):\n",
        "    \"\"\"\n",
        "    CHECKERBOARD Sample points from a 3x3 checkerboard in 2D\n",
        "    \"\"\"\n",
        "    X = np.random.rand(n,2); # samples dimension N = 2\n",
        "    l = np.mod(np.ceil(X*3), 2);\n",
        "    y0 = np.logical_xor(l[:,0], l[:,1])\n",
        "    y = y0*2-1 # {-1,+1}\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blm2jX4Bt-d5"
      },
      "outputs": [],
      "source": [
        "# Usage sample\n",
        "import matplotlib.pyplot as plt\n",
        "X, y = checkerboard(3000)\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.scatter(X[y==1,0], X[y==1,1], c='r', s=3)\n",
        "plt.scatter(X[y==-1,0], X[y==-1,1], c='b', s=3)\n",
        "plt.xlim([0,1])\n",
        "plt.ylim([0,1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-5bHUGQitCY"
      },
      "source": [
        "## Optional Reading Materials\n",
        "1. Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming, 127(1), 3-30. [[pdf](http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf)]\n",
        "\n",
        "2. Sch√∂lkopf, B., Williamson, R. C., Smola, A. J., Shawe-Taylor, J., & Platt, J. C. (2000). Support vector method for novelty detection. In Advances in neural information processing systems (pp. 582-588). [[pdf](http://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf)]\n",
        "\n",
        "3. Livni, R., Crammer, K. & Globerson, A.. (2012). A Simple Geometric Interpretation of SVM using Stochastic Adversaries. Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, in PMLR 22:722-730. [[pdf](http://proceedings.mlr.press/v22/livni12/livni12.pdf)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YACLgVEMt-d7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}