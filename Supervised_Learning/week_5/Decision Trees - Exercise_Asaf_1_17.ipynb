{"cells":[{"cell_type":"markdown","metadata":{"id":"Cg047ujRBmtU"},"source":["# Decision Trees Exercise\n","In this exercise you will show that ID3 is sub-optimal. Implement a simple version of Decision Tree, and will then apply a Decision Tree classsifier on the MNIST hand written digits dataset that we already saw.\n"]},{"cell_type":"markdown","metadata":{"id":"osXAmT5y4iM8"},"source":["## 1. Suboptimality of ID3\n","Consider the following training set, where $\\mathcal{X} = \\{0, 1\\}^3$ and $\\mathcal{Y} =\\{0, 1\\}$:\n","\n","$$\n","\\begin{aligned}\n","((1, 1, 1), 1)\\\\\n","((1, 0, 0), 1)\\\\\n","((1, 1, 0), 0)\\\\\n","((0, 0, 1), 0)\n","\\end{aligned}\n","$$\n","\n","Suppose we wish to use this training set in order to build a decision tree of depth 2 (i.e. for each\n","input we are allowed to ask two questions of the form \"$x_i = 0$?\" before deciding on the label).\n","\n","1. Suppose we run the ID3 algorithm up to depth 2 (namely, we pick the root node and its\n","children according to the algorithm, but instead of keeping on with the recursion, we stop\n","and pick leaves according to the majority label in each subtree, once we reach depth 2). \n","Assume that the subroutine used to measure the quality of each feature is based on the information gain, and that if two features get the same score, one of them is picked arbitrarily. \n","Show that the training error of the resulting decision tree is at least 1/4.\n","2. Find a decision tree of depth 2, which attains zero training error.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xC7Anlwu50XD"},"source":["#### Answer\n","![alt text](https://snipboard.io/DSFTtz.jpg)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xC7Anlwu50XD"},"source":["#### Answer\n","![alt text](https://snipboard.io/DSFTtz.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NODE:\n","   def __init__(self,feature,value,L=None,R=None):\n","    self.feature = feature\n","    self.value = value\n","    self.L = L\n","    self.R = R\n","\n","\n","def fit():\n","    get_f_v()\n","    tree = NODE(f,v)\n","    pass"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["1.8371173070873836"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["np.sqrt( (9+9+36)) /4"]},{"cell_type":"markdown","metadata":{"id":"tLXpoHg64HlD"},"source":["## 2. Implementing Decision Tree From Scratch\n","In this exercise you will need to implement a simple version of Decision Tree from scratch. Your decision tree will handle **continuous input and output** (this should actually work also for binary input attributes).\n","\n","* Compelete the skeleton class below\n","  - `X` is a matrix of data values (rows are samples, columns are attributes)\n","  - `y` is a vector of corresponding target values\n","  - `min_leaf` is the minimal number of samples in each leaf node\n","  \n","* For splitting criterion, use either **\"Train Squared Error Minimization (Reduction in Variance)\"** or **\"Train Absolute Error Minimization\"** (choose one). Whatever you choose, make sure you implement the splitting point decision efficiently (in $O(nlgn)$ time).\n","\n","* The `predict` function will use mean of the target values in the leaf node matching each row of the given `X`. The result is a vector of predictions matching the number of rows in `X`.\n","\n","* To check your decision tree implementation, use the boston dataset (`from sklearn.datasets import load_boston`) split the data set into train and test using (`from sklearn.model_selection import train_test_split`)\n","\n","  - Use the following to estimate what are the best hyper parameters to use for your model\n","```\n","    for min_leaf in [1,5,10,100]:\n","      dt = DecisionTree(X, y, n, sz, min_leaf)\n","      mse = # mean square error over test set\n","      print(\"min_leaf:{0} --- oob mse: {1}\".format(min_leaf, mse))\n","```\n","  \n","  - Using your chosen hyperparameters as a final model, plot the predictions vs. true values of all the samples in the training set . Use something like:\n","  ```\n","  y_hat = dt.predict(X_train)  # forest is the chosen model\n","  plt.scatter(y_hat, y_test)\n","  ```"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# Libraries\n","import pandas as pd\n","from sklearn.datasets import load_boston\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Load the Boston dataset\n","boston_dataset = load_boston()\n","\n","\n","# Convert the dataset into a pandas dataframe\n","boston_df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n","boston_df['target'] = boston_dataset.target\n","from sklearn.model_selection import train_test_split\n","y = boston_df['target']\n","X = boston_df.drop(['target'],axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calc RMSE\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["### # FOR DEBUGGING\n","def find_feat_val(df,y):\n","    RMSE = np.inf\n","    VAL = 0\n","    COL = ''\n","    for colname, column in df.iteritems():\n","        rmse,val = calc_rmse(column,y)\n","        \n","        if RMSE > rmse:\n","            RMSE = rmse\n","            VAL = val\n","            COL = colname\n","\n","    return VAL,COL\n","    \n","\n","def calc_rmse(X,y):\n","    # concatenate feature and target to easily sort\n","    df = pd.concat([X,y],axis=1)\n","    df.sort_values(df.columns[0],inplace=True)\n","\n","    # Create different splits of data and find the one with least RMSE\n","    \n","    best_rmse = np.inf\n","\n","    for indx in range(df.shape[0]-1): ## CAN BE IMPROVED BY ONLY TAKE UNIQUE VALUES OF X\n","        left = df.iloc[:indx+1]\n","        right = df.iloc[indx+1:]\n","        left_mean = left.iloc[:,-1].mean()\n","        right_mean = right.iloc[:,-1].mean()\n","        l_rmse = np.sqrt((left.iloc[:,-1] - left_mean)**2).mean()\n","        r_rmse = np.sqrt((right.iloc[:,-1] - right_mean)**2).mean()\n","        t_rmse = r_rmse * right.shape[0]/df.shape[0] + l_rmse * left.shape[0]/df.shape[0]# Total RMSE\n","\n","        if best_rmse > t_rmse: # if found a better rmse\n","            best_rmse = t_rmse\n","            split_val = df.iloc[indx,0]\n","\n","    \n","    return best_rmse,split_val\n","\n","# def calc_rmse(X,y): # OF chatGPT\n","#     # concatenate feature and target to easily sort\n","#     df = pd.concat([X,y],axis=1)\n","#     df.sort_values(df.columns[0],inplace=True)\n","\n","#     # Create different splits of data and find the one with least RMSE\n","\n","#     best_rmse = np.inf\n","#     unique_x = pd.unique(df[df.columns[0]])\n","#     for indx in unique_x:\n","#         left = df[df[df.columns[0]]<=indx]\n","#         right = df[df[df.columns[0]]>indx]\n","#         left_mean = left.iloc[:,-1].mean()\n","#         right_mean = right.iloc[:,-1].mean()\n","#         l_rmse = np.sqrt((left.iloc[:,-1] - left_mean)**2).mean()\n","#         r_rmse = np.sqrt((right.iloc[:,-1] - right_mean)**2).mean()\n","#         t_rmse = r_rmse * right.shape[0]/df.shape[0] + l_rmse * left.shape[0]/df.shape[0]# Total RMSE\n","\n","#         if best_rmse > t_rmse: # if found a better rmse\n","#             best_rmse = t_rmse\n","#             split_val = indx\n","\n","#     return best_rmse,split_val\n","\n","\n","class NODE(object):\n","    def __init__(self,feature,value,L=None,R=None):\n","        self.feature = feature\n","        self.value = value\n","        self.L = L\n","        self.R = R\n","        ## REPLACE WITH INDICES\n","        self.n_samples = []\n","        self.mean = 0\n","        self.is_root = False"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"QA54r4DiQDkM"},"outputs":[],"source":["class DecisionTree():\n","  def __init__(self, X, y, min_leaf=20):\n","    self.X = X\n","    self.y = y\n","    self.min_leaf = min_leaf\n","    self.Tree = None\n","\n","  class NODE(object):\n","    def __init__(self,feature,value,L=None,R=None):\n","      self.feature = feature\n","      self.value = value\n","      self.L = L\n","      self.R = R\n","      self.n_samples = []\n","      self.mean = 0\n","      self.is_root = False\n","    \n","\n","\n","\n","  def find_feat_val(self,df,y):\n","      RMSE = np.inf\n","      VAL = 0\n","      COL = ''\n","      for colname, column in df.iteritems():\n","          rmse,val = self.calc_rmse(column,y)\n","          \n","          if RMSE > rmse:\n","              RMSE = rmse\n","              VAL = val\n","              COL = colname\n","\n","      return VAL,COL\n","    \n","\n","  def calc_rmse(self,X,y):\n","      # concatenate feature and target to easily sort\n","      df = pd.concat([X,y],axis=1)\n","      df.sort_values(df.columns[0],inplace=True)\n","\n","      # Create different splits of data and find the one with least RMSE\n","      \n","      best_rmse = np.inf\n","\n","      for indx in range(df.shape[0]-1): ## CAN BE IMPROVED BY ONLY TAKE UNIQUE VALUES OF X\n","          left = df.iloc[:indx+1]\n","          right = df.iloc[indx+1:]\n","          left_mean = left.iloc[:,-1].mean()\n","          right_mean = right.iloc[:,-1].mean()\n","          l_rmse = np.sqrt((left.iloc[:,-1] - left_mean)**2).mean()\n","          r_rmse = np.sqrt((right.iloc[:,-1] - right_mean)**2).mean()\n","          t_rmse = r_rmse * right.shape[0]/df.shape[0] + l_rmse * left.shape[0]/df.shape[0]# Total RMSE\n","\n","          if best_rmse > t_rmse: # if found a better rmse\n","              best_rmse = t_rmse\n","              split_val = df.iloc[indx,0]\n","\n","      \n","      return best_rmse,split_val\n","\n","\n","\n","  def fit(self,X,y):\n","    # Find best feature to split on and the value\n","\n","    # split_val,feature = self.find_feat_val(X,y) # REAL\n","    split_val,feature = find_feat_val(X,y) # DEBUG\n","    \n","    # Create new node\n","    if self.Tree is None:\n","      \n","      # node = self.NODE(feature,split_val) # create new node REAL\n","      node = NODE(feature,split_val) # create new node DEBUG\n","      node.is_root = True\n","      self.Tree = True\n","      node.mean = np.mean(y)\n","    else:\n","      if feature:\n","        # node = self.NODE(feature,split_val) # REAL\n","        node = NODE(feature,split_val) # DEBUG\n","        node.mean = np.mean(y)\n","      else: \n","        return\n","\n","\n","    # Create left and right data and target subsets\n","    \n","    subset_l = X[X[feature] <= split_val].drop([feature],axis=1)\n","    \n","    y_l = self.y.loc[subset_l.index]\n","    \n","    subset_r = X[X[feature] > split_val].drop([feature],axis=1)\n","    y_r = self.y.loc[subset_r.index]\n","   \n","   \n","\n","\n","    # Check that there are features and samples to split node in left subset\n","    if (subset_l.shape[0] >= self.min_leaf) & (subset_l.shape[1] > 0): # Check if creating left node is possible  \n","      node.L = self.fit(subset_l,y_l) # below threshold\n","\n","    # else: \n","    #   node.n_samples.append(y_l.values)\n","    #   node.mean = np.mean(np.concatenate(node.n_samples))\n","      \n","      \n","# Check that there are features and samples to split node in right subset\n","    if (subset_r.shape[0] >= self.min_leaf) & (subset_r.shape[1] > 0): \n","      node.R = self.fit(subset_r,y_r) # below threshold\n","    # else:\n","    #   node.n_samples.append(y_r.values)\n","    #   node.mean = np.mean(np.concatenate(node.n_samples))\n","      \n","           \n","\n","    \n","    if node.is_root:\n","      self.Tree = node\n","    else: # node is leaf\n","      # node.mean = np.mean(np.concatenate(node.n_samples)) ## MIGHT BE RDEUNDENT\n","      return node\n","\n","  def predict(self, X):\n","    y_pred = []\n","    \n","    for indx,row in X.iterrows():\n","      curr_node = self.Tree\n","      \n","      can_go_deep = True\n","      while can_go_deep: # while curr_node is not a leaf\n","\n","        if row[curr_node.feature] <= curr_node.value and curr_node.L: # if equal or below splitting value\n","          curr_node = curr_node.L\n","          \n","        elif row[curr_node.feature] > curr_node.value and curr_node.R:\n","          curr_node = curr_node.R\n","          \n","\n","        else: \n","          can_go_deep = False ## break if node is a leaf\n","\n","      y_pred.append(curr_node.mean)   \n","    \n","    return y_pred\n","\n","min_leaf = 20\n","samples = X_train.shape[0]\n","tree = DecisionTree(X_train,y_train,min_leaf)\n","tree.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["y_pred = tree.predict(X_test)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Create an instance of the DecisionTreeClassifier\n","clf = DecisionTreeRegressor(min_samples_leaf=20)\n","\n","# Train the classifier on the training data\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred_sk = clf.predict(X_test)\n","\n","\n"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["25.692560791329345\n","19.009901095819135\n"]}],"source":["print(mean_squared_error(y_pred,y_test))\n","print(mean_squared_error(y_pred_sk,y_test))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n","<__main__.NODE object at 0x000001FC1B040B80>\n","<__main__.NODE object at 0x000001FC1B042970>\n","<__main__.NODE object at 0x000001FC1B0427C0>\n","<__main__.NODE object at 0x000001FC1B05C8E0>\n"]},{"data":{"text/plain":["[1.0, 2.5, 2.5, 45.0, 45.0, 1.0, 1.0, 1.0]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["data = {'x1': [1, 2, 3, 4, 5,1,1,1],\n","        'x2': [5,1,3,2,4,5,5,5],\n","        'x3': [10000,1134,500,100,5000,5,5,5],\n","        'y': [1,2,3,40,50,1,1,1]}\n","df = pd.DataFrame(data)\n","\n","\n","\n","X = df[['x1', 'x2','x3']]\n","y = df['y']"]},{"cell_type":"markdown","metadata":{"id":"TF5TjNuvTKof"},"source":["## 3. Using Decision Tree for Digits Classification\n","Remeber the MNIST dataset used - you will now test the power of decision trees on this problem.\n","This time you are given a free hand in choosing the test and train set sizes, model parameters (such as gain function and constraints over the trees) and features (whether to use binary pixel values or the original continous gray value).\n","  - You can use `sklearn.tree.DecisionTreeClassifier`\n","- Once you are satisfied with the model parameters, plot the importance of each of the pixels to the final decision.\n","- Last, estimate the class assignment probabilities for all the correctly classified and misclassified examples in your test data.\n","- Discuss your results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-k9WpIV_n7Y","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# code and answer go here"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"5e2ac3774ab32f260f9760d004fae9073803f03c7ce8e55cd71c8539f29de38d"}}},"nbformat":4,"nbformat_minor":0}
