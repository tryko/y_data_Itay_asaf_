{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg047ujRBmtU"
   },
   "source": [
    "# Decision Trees Exercise\n",
    "In this exercise you will show that ID3 is sub-optimal. Implement a simple version of Decision Tree, and will then apply a Decision Tree classsifier on the MNIST hand written digits dataset that we already saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXAmT5y4iM8"
   },
   "source": [
    "## Suboptimality of ID3\n",
    "Consider the following training set, where $\\mathcal{X} = \\{0, 1\\}^3$ and $\\mathcal{Y} =\\{0, 1\\}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "((1, 1, 1), 1)\\\\\n",
    "((1, 0, 0), 1)\\\\\n",
    "((1, 1, 0), 0)\\\\\n",
    "((0, 0, 1), 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Suppose we wish to use this training set in order to build a decision tree of depth 2 (i.e. for each\n",
    "input we are allowed to ask two questions of the form \"$x_i = 0$?\" before deciding on the label).\n",
    "\n",
    "1. Suppose we run the ID3 algorithm up to depth 2 (namely, we pick the root node and its\n",
    "children according to the algorithm, but instead of keeping on with the recursion, we stop\n",
    "and pick leaves according to the majority label in each subtree, once we reach depth 2). \n",
    "Assume that the subroutine used to measure the quality of each feature is based on the information gain, and that if two features get the same score, one of them is picked arbitrarily. \n",
    "Show that the training error of the resulting decision tree is at least 1/4.\n",
    "2. Find a decision tree of depth 2, which attains zero training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC7Anlwu50XD"
   },
   "source": [
    "#### Answer\n",
    "The **entropy** of a given node with a sample set $S$ is defined as \n",
    "\n",
    "$$\n",
    "\\Pr(y|S)=p \\quad\\Longrightarrow\\quad H(S)=-p\\log p-(1-p)\\log (1-p)\n",
    "$$\n",
    "\n",
    "\n",
    "The **information gain** is defined as the weighted reduction in entropy.\n",
    "\n",
    "Assume that the set at the current node we wish to split is denoted $S_\\text{parent}$, its subset of samples with property $i$ being 1 is denoted $S_{x_i}$, and its subset of samples with property $i$ being 0 is denoted $S_{\\lnot x_i}$.\n",
    "\n",
    "So, the gain of splitting based on property $i$, it is defined as follows\n",
    "\n",
    "$$\n",
    "\\text{Gain}(S_\\text{parent}, i) = H(S_\\text{parent}) - \\left(\\Pr(x_i|S_\\text{parent})H(S_{x_i}) + \\Pr(\\lnot x_i|S_\\text{parent})H(S_{\\lnot x_i})\\right)\n",
    "$$\n",
    "\n",
    "But the first term is constant for all attributes $i$, so we just need to choose the attribute $i$ which minimizes the weighted entropy of the sub-sets. Let's mark this as $H_\\mathrm{w}(S,i)$, so \n",
    "\n",
    "$$\n",
    "\\text{Gain}(S_\\text{parent}, i) = H(S_\\text{parent}) - H_\\mathrm{w}(S_\\text{parent},i)\n",
    "$$\n",
    "\n",
    "and we want to find in each split \n",
    "$$i = \\arg\\min_i H_\\mathrm{w}(S_\\text{parent},i)$$\n",
    "\n",
    "Last, we note that if the subset is completly pure (all labels of the subset are the same), the entropy is defined as zero.\n",
    "\n",
    "---\n",
    "1.   Root split:\n",
    "\n",
    "  We have that\n",
    "  $$\n",
    "  H_\\mathrm{w}(S_\\text{root},1) = \\tfrac{3}{4}(-\\tfrac{2}{3}\\log \\tfrac{2}{3} - \\tfrac{1}{3}\\log \\tfrac{1}{3}) + \\tfrac{1}{4}(0) \\approx 0.688\\\\\n",
    "  H_\\mathrm{w}(S_\\text{root},2) = H_\\mathrm{w}(S_\\text{root},3) = \\tfrac{1}{2}(-\\tfrac{1}{2}\\log \\tfrac{1}{2} - \\tfrac{1}{2}\\log \\tfrac{1}{2}) + \\tfrac{1}{2}(-\\tfrac{1}{2}\\log \\tfrac{1}{2} - \\tfrac{1}{2}\\log \\tfrac{1}{2})=1\n",
    "  $$\n",
    "\n",
    "  and since $H_\\mathrm{w}(S_\\text{root},1) < H_\\mathrm{w}(S_\\text{root},2) = H_\\mathrm{w}(S_\\text{root},3)$ we choose to first split on attribute 1.\n",
    "\n",
    "\n",
    "2.   Now, we have two sets: $S_{\\lnot x_1}$ which contains only one sample (so we will not split it anymore (it is pure by definition) - if you reach that leaf you will get the label 0. The other set is $S_{x_1}$ which contains all the samples with first coordinate set to 1. Let's check how we will split this node.\n",
    "\n",
    "  $$\n",
    "  H_\\mathrm{w}(S_{x_1},2) = \\tfrac{2}{3}(-\\tfrac{1}{2}\\log \\tfrac{1}{2} - \\tfrac{1}{2}\\log \\tfrac{1}{2}) + \\tfrac{1}{3}(0) = \\tfrac{2}{3}\\\\\n",
    "  H_\\mathrm{w}(S_{x_1},3) = \\tfrac{1}{3}(0) + \\tfrac{2}{3}(-\\tfrac{1}{2}\\log \\tfrac{1}{2} - \\tfrac{1}{2}\\log \\tfrac{1}{2}) = \\tfrac{2}{3}\n",
    "  $$\n",
    "  \n",
    "  This is the same, so we can randomly choose to split on attribute 2 (one of the nodes will have one sample, and hence also one label, while the other node will have two samples with different labels).\n",
    "\n",
    "3. There are no more splits since we got to depth 2 as requiered.\n",
    "\n",
    "---\n",
    "Now, let's test what is the training error of the tree we created. The train error is simply the number of samples in terminal nodes which have a label other than the majority vote in this leaf. In our case, we have 3 terminal nodes, two of which hold one sample (hence are pure and have no training error), and one terminal node which has two samples in it: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "((1, 1, 1), 1)\\\\\\\n",
    "((1, 1, 0), 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this case, the label for this terminal node is picked arbitrarily to be either 0 or 1, but whatever we choose, we will be left with one example on which we make an error. So the overall train error rate for this tree is $1/4$ (as was required to show).\n",
    "\n",
    "---\n",
    "Last, let's describe a tree of depth 2 which has zero train error rate:\n",
    "\n",
    "  *\"First, split on attribute 2, then, in each sub-node, split on attribute 3. Ignore attribute 1 completely.\"*\n",
    "\n",
    "Verify that indeed this tree has zero train error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1872,
     "status": "ok",
     "timestamp": 1611434732172,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "fR1RJoi4OBJz",
    "outputId": "9a2157b8-5626-4afb-bc80-babde726c1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6887218755408672 1.0 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "hw1 = 3/4 * (-2/3 * np.log2(2/3) - 1/3 * np.log2(1/3)) + 1/4*0\n",
    "hw2 = 1/2 * (-1/2 * np.log2(1/2) - 1/2 * np.log2(1/2)) * 2\n",
    "hw3 = 2/3 * (-1/2 * np.log2(1/2) - 1/2 * np.log2(1/2)) + 1/3*0\n",
    "print(hw1, hw2, hw3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLXpoHg64HlD"
   },
   "source": [
    "## Implementing Decision Tree From Scratch\n",
    "In this exercise you will need to implement a simple version of Random Forest from scratch. Your decision tree will handle **continuous input and output** (this should actually work also for binary input attributes). \n",
    "\n",
    "* Compelete the skeleton class below (hint: you should also create a `DecisionTree` class that the `TreeEnsemble` will use)\n",
    "  - `X` is a matrix of data values (rows are samples, columns are attributes)\n",
    "  - `y` is a vector of corresponding target values\n",
    "  - `min_leaf` is the minimal number of samples in each leaf node of each tree in the forest\n",
    "  \n",
    "* For splitting criterion, use either **\"Train Squared Error Minimization (Reduction in Variance)\"** or **\"Train Absolute Error Minimization\"** (choose one). Whatever you choose, make sure you implement the splitting point decision efficiently (in $O(n\\log n)$ time, where $n$ is the number of samples in the current node).\n",
    "\n",
    "* The `predict` function will use mean of the target values in the leaf node matching each row of the given `X`. The result is a vector of predictions matching the number of rows in `X`.\n",
    "\n",
    "* To check your random forest implementation, use the bostom dataset (`from sklearn.datasets import load_boston`)\n",
    "\n",
    "  - Use the following to estimate what are the best hyper parameters to use for your model\n",
    "```\n",
    "    for min_leaf in [1,5]:\n",
    "      forest = TreeEnsemble(X, y, n, sz, min_leaf)\n",
    "      mse = forest.oob_mse()\n",
    "      print(\"n_trees:{0}, sz:{1}, min_leaf:{2} --- oob mse: {3}\".format(n, sz, min_leaf, mse))\n",
    "```\n",
    "  \n",
    "  - Using your chosen hyperparameters as a final model, plot the predictions vs. true values of all the samples in the training set . Use something like:\n",
    "  ```\n",
    "  y_hat = forest.predict(X)  # forest is the chosen model\n",
    "  plt.scatter(y_hat, y)\n",
    "  ```\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Js-GI2eMOr7"
   },
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "executionInfo": {
     "elapsed": 2349,
     "status": "ok",
     "timestamp": 1611434732656,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "_GTTE4uV4UY6"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "  def __init__(self, X, y, attribute_names, min_leaf=5, reset_node_id=False):\n",
    "    self.X, self.y, self.attribute_names, self.min_leaf = \\\n",
    "      X, y, attribute_names, min_leaf\n",
    "    self.n, self.c = X.shape\n",
    "    assert self.c == len(attribute_names)\n",
    "    self.value = np.mean(y)\n",
    "    self.score = float('inf')\n",
    "    self._find_varsplit()\n",
    "    \n",
    "  def _find_varsplit(self):\n",
    "    for j in range(self.c): \n",
    "      self._find_better_split(j)\n",
    "    if self.score == float('inf'): \n",
    "      return\n",
    "    Xj = self.split_col\n",
    "    lhs = np.nonzero(Xj <= self.split_value)[0]\n",
    "    rhs = np.nonzero(Xj > self.split_value)[0]\n",
    "    self.lhs = DecisionTree(self.X[lhs], self.y[lhs], self.attribute_names)\n",
    "    self.rhs = DecisionTree(self.X[rhs], self.y[rhs], self.attribute_names)\n",
    "\n",
    "  def _find_better_split(self, attribute_idx):\n",
    "    # x is the j'th column (j = attribute_idx)\n",
    "    x, y = self.X[:, attribute_idx], self.y\n",
    "    sort_idx = np.argsort(x)\n",
    "    sort_y, sort_x = y[sort_idx], x[sort_idx]\n",
    "    rhs_cnt, rhs_sum, rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "    lhs_cnt, lhs_sum, lhs_sum2 = 0, 0., 0.\n",
    "\n",
    "    for i in range(0, self.n - self.min_leaf - 1):\n",
    "      # set the i'th sample, in the j'th arrtibute\n",
    "      xi, yi = sort_x[i], sort_y[i]  \n",
    "      # apply incremental updates to save score computation time\n",
    "      lhs_cnt += 1; rhs_cnt -= 1\n",
    "      lhs_sum += yi; rhs_sum -= yi\n",
    "      lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "      # calculate current split point i if needed\n",
    "      if i < self.min_leaf or xi == sort_x[i+1]:\n",
    "          continue\n",
    "      lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "      rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "      curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "      if curr_score < self.score: \n",
    "        self.attribute_idx, self.score, self.split_value = attribute_idx, curr_score, xi\n",
    "\n",
    "  def _predict_row(self, xi):\n",
    "    if self.is_leaf:\n",
    "      return self.value\n",
    "    t = self.lhs if xi[self.attribute_idx] <= self.split_value else self.rhs\n",
    "    return t._predict_row(xi)\n",
    "  \n",
    "  def predict(self, X):\n",
    "    return np.array([self._predict_row(xi) for xi in X])\n",
    "  \n",
    "  @property\n",
    "  def split_name(self): return self.attribute_names[self.attribute_idx]\n",
    "\n",
    "  @property\n",
    "  def split_col(self): return self.X[:, self.attribute_idx]\n",
    "\n",
    "  @property  # TODO: could return a heatmap color based on self.value\n",
    "  def color(self): return '#e58139ff' if self.is_leaf else '#eff13955'\n",
    "  \n",
    "  @property\n",
    "  def is_leaf(self): return self.score == float('inf')\n",
    "\n",
    "  def __str__(self):\n",
    "    \"\"\" Representation of the root node of this tree \"\"\"\n",
    "    if self.is_leaf:\n",
    "      s = 'LEAF'\n",
    "    else:\n",
    "      s = f'{self.split_name} <= {self.split_value:.2f}\\\\nscore = {self.score:.2f}'\n",
    "    s += f'\\\\nsamples = {self.n}\\\\nvalue = {self.value:.2f}'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1611435097010,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "pzUoN4mz84iR",
    "outputId": "07bbbabe-8ba8-4195-f024-29624a6401f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Boston: 506 samples with 13 attributes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "# X = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "attribute_names = boston.feature_names\n",
    "print(\"Loaded Boston:\", X.shape[0],\"samples with\",X.shape[1],\"attributes\")\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103781,
     "status": "ok",
     "timestamp": 1611435452327,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "pPzrMjuPCWHx",
    "outputId": "53616269-fe01-4260-d65e-f88af0c400b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_leaf:1 --- mse: 25.152522295037045\n",
      "min_leaf:5 --- mse: 25.152522295037045\n",
      "min_leaf:10 --- mse: 25.152522295037045\n",
      "min_leaf:100 --- mse: 17.644934876310987\n"
     ]
    }
   ],
   "source": [
    "# Applying manual grid search to find optimal params using mse\n",
    "# (could also use sklearn.model_selection.GridSearchCV)\n",
    "for min_leaf in [1,5,10,100]:\n",
    "      dt = DecisionTree(X_train, y_train, attribute_names, min_leaf)\n",
    "      mse = sum((dt.predict(X_test) - y_test) ** 2) / len(X_test)\n",
    "      print(\"min_leaf:{0} --- mse: {1}\".format(min_leaf, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 108199,
     "status": "ok",
     "timestamp": 1611435460411,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "pSWD4W6NPIvP",
    "outputId": "5cc8a360-4b43-4654-a0be-1cb1eb0991b5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the performance of the chosen model\n",
    "dt = DecisionTree(X_train, y_train, attribute_names, min_leaf=100)\n",
    "y_hat = dt.predict(X_test)\n",
    "plt.figure()\n",
    "plt.scatter(y_hat, y_test, alpha=0.5, s=10)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('true outcome');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF5TjNuvTKof"
   },
   "source": [
    "## Using Decision Tree \n",
    "Remeber the MNIST dataset used - you will now test the power of decision trees on this problem.\n",
    "This time you are given a free hand in choosing the test and train set sizes, model parameters (such as gain function and constraints over the trees) and features (whether to use binary pixel values or the original continous gray value).\n",
    "- Choose which model parameters you wish to optimize, explain how would you do that, and find a model which you believe would have the minimal generalization error --- do this for both a single decision tree model, and a random forest.\n",
    "  - You can use `sklearn.tree.DecisionTreeClassifier`\n",
    "- Once you are satisfied with the model parameters, plot for each of the models (a single decision tree and random forest) the importance of each of the pixels to the final decision.\n",
    "- Last, estimate the class assignment probabilities for all the correctly classified and misclassified examples in your test data.\n",
    "- Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2FybnkDTOVv"
   },
   "source": [
    "## Answer\n",
    "The way to optimize the hyper-parameters is to use a validation set. For this, we can use for example the score of our model on a 5-fold cross-validation test perfromed on our train data. Below are the results.\n",
    "Due to a relatively long running time, we use only 2000 sample. Using a grid search over a few hyperparameters values and reviewin their 5-fold CV scores, we see the difference between the models is very low. However, based on the maximal score, we chose to use the 'entropy' criterion, with max depth of 32, and 200 as the max number of features (a better test would be to run the CV several times and on much bigger sample set). Once we trained the model over the entire train data set, this resulted in a totoal accuracy of ~0.87 (not too bad, but not too good either).\n",
    "\n",
    "For the Random Forest we decided only to try and optimize the number of trees in the forest and the criterion, and to use the default values for all other hyperparameters. Looking at the 5-fold CV scores, we decided to use the 'gini' criterion with 200 trees in the forest (where running time was also a consideration). We then train the model over the entire train data set, which resulted in a very good total accuracy of ~0.97. This shows how powerful predictors random forests are.\n",
    "\n",
    "When plotting the pixels importance we can see that both algorithms found the center much more important than the pixels near the edges, and we can see a few specific areas which were much more critical to reach the final predicition (specifically, a \"line\" going from the center towards the bottom left corner, and two spots to the left and above this line).\n",
    "However, the random forest model found many more important pixels - this is probably since those features are important to reach the final prediction once we need to get the correct classification also for the \"hard\" cases.\n",
    "\n",
    "Reminder: \"The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest, where the class probability of a single tree is the fraction of samples of the same class in a leaf.\" (see [`RandomForestClassifier.predict_proba`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)).\n",
    "So, using the estimated class probability, for both the error classifications and the correct classifications, we see that the classifier clearly has low confidence in the results which turned out to be error predictions. The probablity of a class assignment which turned out to be a missclassification was 0.41 (+/- 0.07), compared with the probability of a correct class assignment, which was 0.84 (+/- 0.09)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV3-bZPJTR_H"
   },
   "source": [
    "### Loading the data\n",
    "Same as we did in previous exercise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1611436795693,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "oBwl1a0hRxAa"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "# Optinally, set data_home to where you want to download your data\n",
    "mnist = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1611436820980,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "jftsVQ_ZTFo9"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_digit_image(digit_data, digit_label=None, title=\"\"):\n",
    "  plt.imshow(np.reshape(digit_data, (8,8)), cmap=plt.cm.gray)\n",
    "  plt.title(title, fontsize = 12)\n",
    "  plt.axis('off')\n",
    "  plt.grid(False)\n",
    "\n",
    "def calc_confusion_matrix(true_lables, predicted_labels, normalize=False, show_plot=True, title=\"\"):\n",
    "  cm = confusion_matrix(true_lables, predicted_labels)\n",
    "\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "  if show_plot:\n",
    "    num_labels = np.unique([true_lables, predicted_labels]).size\n",
    "    plt.figure(figsize=(num_labels, num_labels))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".3f\" if normalize else \"d\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    plt.title(title, size = 15)\n",
    "   \n",
    "  return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1611436824258,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "iPZaUgbbTaO1",
    "outputId": "0ba3307e-4eb5-4a76-bd87-cfa9a1084020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape(1797, 64)\n",
      "Label Data Shape(1797,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print to image data and labels data shapes\n",
    "print(\"Image Data Shape\" + str(mnist.data.shape))\n",
    "print(\"Label Data Shape\" + str(mnist.target.shape))\n",
    "\n",
    "# Print a sample of each digits\n",
    "plt.figure(figsize=(14,2))\n",
    "for digit in range(10):\n",
    "  first_item_index = np.argwhere(mnist.target==digit)[0]\n",
    "  (image, label) = (mnist.data[first_item_index], mnist.target[first_item_index])\n",
    "  plt.subplot(1, 10, digit + 1)\n",
    "  show_digit_image(image, label, title='Label: %i' % digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1611436835334,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "TCPWvJ52TMEw"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (we set random_state so that we fix\n",
    "# the splitting, so not every time we run this notebook a different split would be made)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(mnist.data, mnist.target, test_size=1/7.0, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2K1mL6SUEbT"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5338,
     "status": "ok",
     "timestamp": 1611436842557,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "TZqFc9nCUC-N",
    "outputId": "7a276ee2-6abf-47a7-b326-bce7012ced5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv5 score mean = 0.8564563391419974 for criterion gini max_depth=8 max_features=10\n",
      "cv5 score mean = 0.843994786258173 for criterion gini max_depth=8 max_features=20\n",
      "cv5 score mean = 0.8537427427664163 for criterion gini max_depth=8 max_features=40\n",
      "cv5 score mean = 0.8452586293347872 for criterion gini max_depth=8 max_features=64\n",
      "cv5 score mean = 0.8544636104925687 for criterion gini max_depth=16 max_features=10\n",
      "cv5 score mean = 0.8563176951154267 for criterion gini max_depth=16 max_features=20\n",
      "cv5 score mean = 0.8602443581739587 for criterion gini max_depth=16 max_features=40\n",
      "cv5 score mean = 0.8388542852603663 for criterion gini max_depth=16 max_features=64\n",
      "cv5 score mean = 0.8564396577690694 for criterion gini max_depth=24 max_features=10\n",
      "cv5 score mean = 0.8472216937551285 for criterion gini max_depth=24 max_features=20\n",
      "cv5 score mean = 0.8531108464294883 for criterion gini max_depth=24 max_features=40\n",
      "cv5 score mean = 0.8441072735425628 for criterion gini max_depth=24 max_features=64\n",
      "cv5 score mean = 0.8375830911588285 for criterion gini max_depth=32 max_features=10\n",
      "cv5 score mean = 0.8505320762028872 for criterion gini max_depth=32 max_features=20\n",
      "cv5 score mean = 0.8478541215989315 for criterion gini max_depth=32 max_features=40\n",
      "cv5 score mean = 0.8511206922240822 for criterion gini max_depth=32 max_features=64\n",
      "cv5 score mean = 0.8498337860629702 for criterion entropy max_depth=8 max_features=10\n",
      "cv5 score mean = 0.8485291045107566 for criterion entropy max_depth=8 max_features=20\n",
      "cv5 score mean = 0.8473011853809542 for criterion entropy max_depth=8 max_features=40\n",
      "cv5 score mean = 0.8531020202447619 for criterion entropy max_depth=8 max_features=64\n",
      "cv5 score mean = 0.8452585885922206 for criterion entropy max_depth=16 max_features=10\n",
      "cv5 score mean = 0.8485432140726848 for criterion entropy max_depth=16 max_features=20\n",
      "cv5 score mean = 0.8525411121107835 for criterion entropy max_depth=16 max_features=40\n",
      "cv5 score mean = 0.8472765781006378 for criterion entropy max_depth=16 max_features=64\n",
      "cv5 score mean = 0.8426838016905969 for criterion entropy max_depth=24 max_features=10\n",
      "cv5 score mean = 0.8452772562836399 for criterion entropy max_depth=24 max_features=20\n",
      "cv5 score mean = 0.8479215713885022 for criterion entropy max_depth=24 max_features=40\n",
      "cv5 score mean = 0.8590333184741142 for criterion entropy max_depth=24 max_features=64\n",
      "cv5 score mean = 0.8525136509222326 for criterion entropy max_depth=32 max_features=10\n",
      "cv5 score mean = 0.8383094863278254 for criterion entropy max_depth=32 max_features=20\n",
      "cv5 score mean = 0.848581742518091 for criterion entropy max_depth=32 max_features=40\n",
      "cv5 score mean = 0.8466370689579799 for criterion entropy max_depth=32 max_features=64\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import StringIO, pydot\n",
    "\n",
    "num_sample = 2000\n",
    "\n",
    "for criterion in ['gini', 'entropy']:\n",
    "  for max_depth in [8, 16, 24, 32]:\n",
    "    for max_features in [10, 20, 40, 64]:\n",
    "      clf = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=32, max_features=63)\n",
    "      clf = clf.fit(train_img[:num_sample], train_lbl[:num_sample])\n",
    "      scores = cross_val_score(clf, train_img[:num_sample], train_lbl[:num_sample].tolist(), cv=5)\n",
    "      print('cv5 score mean = ' + str(scores.mean()) + ' for criterion ' + criterion + ' max_depth=' + str(max_depth)+ ' max_features=' + str(max_features))\n",
    "      #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() / 2)) # Cross Validation Results for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1611436946702,
     "user": {
      "displayName": "Slava Mironov",
      "photoUrl": "",
      "userId": "13726909368929337564"
     },
     "user_tz": -120
    },
    "id": "kKgNFRR0XytB",
    "outputId": "0093da43-6343-4207-bef7-a9f71936d5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.9412    0.8649        17\n",
      "           1     0.7097    0.8462    0.7719        26\n",
      "           2     0.9545    0.8400    0.8936        25\n",
      "           3     0.7143    0.9091    0.8000        22\n",
      "           4     0.8000    1.0000    0.8889        16\n",
      "           5     0.8519    0.7931    0.8214        29\n",
      "           6     0.9143    0.9697    0.9412        33\n",
      "           7     0.7667    0.7419    0.7541        31\n",
      "           8     0.9286    0.4815    0.6341        27\n",
      "           9     0.8000    0.7742    0.7869        31\n",
      "\n",
      "   micro avg     0.8171    0.8171    0.8171       257\n",
      "   macro avg     0.8240    0.8297    0.8157       257\n",
      "weighted avg     0.8286    0.8171    0.8121       257\n",
      "\n",
      "Totoal accuracy: 0.8171206225680934\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final decision tree model\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=32, max_features=64)\n",
    "clf = clf.fit(train_img, train_lbl)\n",
    "predictions = clf.predict(test_img)\n",
    "print(metrics.classification_report(test_lbl.tolist(), predictions, digits=4))\n",
    "print('Totoal accuracy: ' + str(clf.score(test_img, test_lbl)))\n",
    "\n",
    "# Pixel importances on 8*8 image\n",
    "importances = clf.feature_importances_\n",
    "importances = importances.reshape((8, 8))\n",
    "\n",
    "# Plot pixel importances\n",
    "plt.matshow(importances, cmap=plt.cm.hot)\n",
    "plt.colorbar()\n",
    "plt.title(\"Pixel importances for decision tree\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Trees Solution 2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}