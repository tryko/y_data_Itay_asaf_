{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7jn3RRSSGbb"
      },
      "source": [
        "# Probabilistic classifier of texts into spam / ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XGIBQPBSGbi"
      },
      "source": [
        "## Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GArFA9f2SGbk"
      },
      "source": [
        "Here is a classical \"complete the notebook\" assignment. \n",
        "\n",
        "You can run all the cells in the notebook, and some of them you have to complete. \n",
        "\n",
        "The code you have to complete is marked with `#TODO` comments. The cells containing such code also contain assertions that you should fulfill. \n",
        "\n",
        "If the cells produce no errors, you can be pretty sure you do everything OK. \n",
        "\n",
        "Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UoWcpibiSGbu"
      },
      "outputs": [],
      "source": [
        "def square_root(x):\n",
        "    \"\"\" This is a function that takes a non-negative numeric argument x and produces its square root. \"\"\"\n",
        "    # TODO: calculate the square root of x and put it into the y variable instead of None. \n",
        "    # If you are not sure, have a look on the list of Python basic operators\n",
        "    # https://www.tutorialspoint.com/python/python_basic_operators.htm\n",
        "    y = x**0.5\n",
        "    return y\n",
        "\n",
        "assert square_root(144) == 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhdue65hSGb8"
      },
      "source": [
        "Now that you understand the format, let's have look at a [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) of short messages into spam and not-spam.\n",
        "\n",
        "The main idea behind it is that $$P(spam|text) = \\frac{P(spam)P(text|spam)}{P(text)}$$\n",
        "\n",
        "You will have to implement this formula along with some hacks to make its application more robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA3FpI3BSGb_"
      },
      "source": [
        "![](https://pics.me.me/suppose-you-have-one-rabbit-now-suppose-someone-gives-you-21826742.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bb-0QvpSGcA"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbW707TKUlqq"
      },
      "source": [
        "The cell below loads the file with messages. \n",
        "\n",
        "If you run this notebook locally on Windows, you have to download the file manually. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AmmPwJzlSRm9",
        "outputId": "a29853f5-89e5-4b01-d160-6e691bdd9e54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/avidale/ps4ds2019/master/homework/week1/spam_classifier/SMSSpamCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX3xYoFQSGcC"
      },
      "source": [
        "The following cell imports some Python libraries. It is possible that you have some of them not installed (namely, `pandas`). In this case, you have to install them using package manager from command line. The command would look like `pip install pandas` or `conda install pandas`.\n",
        "\n",
        "If you run this notebook from Google Colab, then the libraries are already installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Tld2ItvwSGcD"
      },
      "outputs": [],
      "source": [
        "# load some useful Python libratries\n",
        "\n",
        "import pandas as pd # the library for working with data tables\n",
        "import re\n",
        "from collections import Counter # a class for counting objects (words and text labels, in our case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "_yol8UpISGcG",
        "outputId": "a57f5c2f-36a0-4c68-dc22-0b6439475be9"
      },
      "outputs": [],
      "source": [
        "# load the data from disk to a tabular format, and give readable names to its columns\n",
        "data = pd.read_csv('./SMSSpamCollection.csv', sep='\\t', header=None)\n",
        "data.columns = ['target', 'text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dAaMxDESGcP"
      },
      "source": [
        "In this dataset, \"ham\" is a good text, and \"spam\" is, well, spam. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "OPRYIT4uSGcQ",
        "outputId": "058af54d-9db7-4f01-8ab9-a9feb37e5cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5572, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  target  \\\n",
              "0    ham   \n",
              "1    ham   \n",
              "2   spam   \n",
              "3    ham   \n",
              "4    ham   \n",
              "\n",
              "                                                                                                                                                          text  \n",
              "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
              "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
              "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
              "3                                                                                                            U dun say so early hor... U c already then say...  \n",
              "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# enable pandas to display large texts and look into our data\n",
        "pd.options.display.max_colwidth = 300\n",
        "\n",
        "print(data.shape) # number of rows and columns\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UkoK7Y5SGcW"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RJ3Q_BBSGcZ"
      },
      "source": [
        "In a minute we will have to estimate probabilites of different texts. \n",
        "\n",
        "We could use *language models* using e.g. n-grams or recurrent neural networks, to calculate probability of original texts. \n",
        "\n",
        "But for our problem, it will be sufficient to represent each text with the set of words (and other symbols) that occur in it. This representation ignores word order and number of words.\n",
        "\n",
        "That is, we will not make difference between texts \n",
        "\n",
        "> this one is a long message. \n",
        "\n",
        "and \n",
        "\n",
        "> this message is a long long long long long long one.\n",
        "\n",
        "Both will be represented as a set of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0jl-tp0jSGcd",
        "outputId": "31ba75d2-4273-4eb3-cd38-bdd4f5c7be4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a', 'is', 'long', 'message', 'one', 'this'}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_words(text):\n",
        "    \"\"\" This function converts the given text into an unordered and uncounted bag of words. \"\"\"\n",
        "    return set(re.split('\\W+', text.lower())).difference({''})\n",
        "\n",
        "# just an example\n",
        "get_words(\"This message is a long, long, long, long long long one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Gs7xsLSGch"
      },
      "source": [
        "This simplified approach will allow us to train the probabilistic model of texts using a modest amount of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vJbmpGfSSGci"
      },
      "outputs": [],
      "source": [
        "# apply this logic to texts of all messages\n",
        "bags_of_words = [get_words(text) for text in data.text]\n",
        "length_texts  = [len(text) for text in data.text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woVdtF5PSGcm"
      },
      "source": [
        "To evaluate how well our model classifies messages, let's train it on the first 3000 texts, and measure accuracy on the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 3000\n",
        "\n",
        "len_train_x, len_test_x, len_train_y, len_test_y = length_texts[:n_train], length_texts[n_train:], data.target[:n_train], data.target[n_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "A2iDe2FLSGcn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        ham\n",
              "1        ham\n",
              "2       spam\n",
              "3        ham\n",
              "4        ham\n",
              "        ... \n",
              "2995     ham\n",
              "2996     ham\n",
              "2997     ham\n",
              "2998     ham\n",
              "2999     ham\n",
              "Name: target, Length: 3000, dtype: object"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_train = 3000\n",
        "\n",
        "train_x, test_x, train_y, test_y = bags_of_words[:n_train], bags_of_words[n_train:], data.target[:n_train], data.target[n_train:]\n",
        "train_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWrcoUWSGcu"
      },
      "source": [
        "## The basic classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t_aDhsOSGcv"
      },
      "source": [
        "In the cell below, we will count occurences of words under different labels.\n",
        "\n",
        "We are going to use `Counter` objects. If you are not sure how they work, please look at [the documentation](https://docs.python.org/3.6/library/collections.html#collections.Counter). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'spam': Counter({'155': 21,\n",
              "          '147': 8,\n",
              "          '157': 16,\n",
              "          '154': 13,\n",
              "          '136': 9,\n",
              "          '149': 10,\n",
              "          '158': 24,\n",
              "          '172': 2,\n",
              "          '120': 3,\n",
              "          '161': 11,\n",
              "          '152': 17,\n",
              "          '159': 23,\n",
              "          '78': 1,\n",
              "          '137': 4,\n",
              "          '144': 5,\n",
              "          '156': 19,\n",
              "          '125': 3,\n",
              "          '162': 9,\n",
              "          '124': 2,\n",
              "          '145': 9,\n",
              "          '72': 1,\n",
              "          '142': 3,\n",
              "          '150': 5,\n",
              "          '121': 4,\n",
              "          '133': 4,\n",
              "          '146': 9,\n",
              "          '138': 3,\n",
              "          '128': 1,\n",
              "          '111': 3,\n",
              "          '148': 9,\n",
              "          '165': 3,\n",
              "          '129': 3,\n",
              "          '49': 2,\n",
              "          '106': 3,\n",
              "          '175': 1,\n",
              "          '160': 16,\n",
              "          '169': 2,\n",
              "          '89': 1,\n",
              "          '96': 1,\n",
              "          '69': 2,\n",
              "          '99': 3,\n",
              "          '126': 2,\n",
              "          '86': 1,\n",
              "          '135': 4,\n",
              "          '140': 3,\n",
              "          '130': 5,\n",
              "          '143': 9,\n",
              "          '97': 1,\n",
              "          '59': 1,\n",
              "          '93': 1,\n",
              "          '153': 11,\n",
              "          '37': 1,\n",
              "          '101': 4,\n",
              "          '131': 2,\n",
              "          '102': 1,\n",
              "          '65': 1,\n",
              "          '95': 1,\n",
              "          '141': 6,\n",
              "          '134': 2,\n",
              "          '132': 5,\n",
              "          '127': 1,\n",
              "          '139': 3,\n",
              "          '41': 1,\n",
              "          '104': 2,\n",
              "          '90': 1,\n",
              "          '115': 2,\n",
              "          '50': 1,\n",
              "          '122': 4,\n",
              "          '81': 2,\n",
              "          '66': 1,\n",
              "          '163': 4,\n",
              "          '151': 5,\n",
              "          '71': 2,\n",
              "          '73': 1,\n",
              "          '114': 2,\n",
              "          '223': 1,\n",
              "          '33': 3,\n",
              "          '84': 1,\n",
              "          '74': 1,\n",
              "          '103': 1,\n",
              "          '118': 2,\n",
              "          '30': 1,\n",
              "          '181': 1,\n",
              "          '48': 2,\n",
              "          '183': 1,\n",
              "          '166': 1,\n",
              "          '119': 2,\n",
              "          '82': 1,\n",
              "          '109': 1,\n",
              "          '116': 1,\n",
              "          '167': 1,\n",
              "          '70': 1,\n",
              "          '173': 1,\n",
              "          '24': 1,\n",
              "          '123': 1,\n",
              "          '110': 1,\n",
              "          '52': 1,\n",
              "          '100': 1}),\n",
              " 'ham': Counter({'111': 5,\n",
              "          '29': 42,\n",
              "          '49': 24,\n",
              "          '61': 14,\n",
              "          '77': 15,\n",
              "          '160': 13,\n",
              "          '109': 8,\n",
              "          '196': 1,\n",
              "          '35': 31,\n",
              "          '26': 52,\n",
              "          '81': 13,\n",
              "          '56': 21,\n",
              "          '41': 38,\n",
              "          '47': 35,\n",
              "          '52': 26,\n",
              "          '88': 15,\n",
              "          '57': 18,\n",
              "          '144': 8,\n",
              "          '30': 41,\n",
              "          '134': 4,\n",
              "          '75': 12,\n",
              "          '64': 18,\n",
              "          '130': 7,\n",
              "          '189': 1,\n",
              "          '84': 20,\n",
              "          '122': 9,\n",
              "          '28': 46,\n",
              "          '27': 44,\n",
              "          '155': 12,\n",
              "          '82': 13,\n",
              "          '142': 11,\n",
              "          '19': 7,\n",
              "          '72': 15,\n",
              "          '32': 54,\n",
              "          '45': 38,\n",
              "          '31': 42,\n",
              "          '67': 21,\n",
              "          '148': 11,\n",
              "          '58': 23,\n",
              "          '124': 7,\n",
              "          '80': 16,\n",
              "          '289': 2,\n",
              "          '76': 18,\n",
              "          '34': 41,\n",
              "          '22': 67,\n",
              "          '40': 28,\n",
              "          '108': 7,\n",
              "          '48': 25,\n",
              "          '25': 53,\n",
              "          '110': 9,\n",
              "          '46': 35,\n",
              "          '42': 34,\n",
              "          '20': 9,\n",
              "          '43': 23,\n",
              "          '73': 24,\n",
              "          '50': 32,\n",
              "          '36': 37,\n",
              "          '14': 6,\n",
              "          '55': 21,\n",
              "          '121': 12,\n",
              "          '195': 2,\n",
              "          '141': 11,\n",
              "          '107': 12,\n",
              "          '33': 34,\n",
              "          '51': 33,\n",
              "          '178': 1,\n",
              "          '183': 4,\n",
              "          '44': 25,\n",
              "          '95': 15,\n",
              "          '115': 11,\n",
              "          '96': 10,\n",
              "          '143': 8,\n",
              "          '156': 13,\n",
              "          '86': 7,\n",
              "          '53': 28,\n",
              "          '38': 37,\n",
              "          '244': 1,\n",
              "          '9': 6,\n",
              "          '39': 29,\n",
              "          '85': 14,\n",
              "          '59': 21,\n",
              "          '384': 1,\n",
              "          '157': 12,\n",
              "          '74': 8,\n",
              "          '94': 10,\n",
              "          '105': 8,\n",
              "          '65': 19,\n",
              "          '146': 5,\n",
              "          '66': 19,\n",
              "          '126': 10,\n",
              "          '159': 12,\n",
              "          '23': 50,\n",
              "          '24': 57,\n",
              "          '152': 9,\n",
              "          '185': 2,\n",
              "          '37': 43,\n",
              "          '92': 14,\n",
              "          '131': 8,\n",
              "          '174': 3,\n",
              "          '63': 25,\n",
              "          '161': 3,\n",
              "          '119': 12,\n",
              "          '69': 10,\n",
              "          '165': 3,\n",
              "          '145': 7,\n",
              "          '83': 10,\n",
              "          '78': 13,\n",
              "          '179': 8,\n",
              "          '97': 4,\n",
              "          '3': 3,\n",
              "          '147': 6,\n",
              "          '8': 4,\n",
              "          '7': 5,\n",
              "          '4': 3,\n",
              "          '21': 6,\n",
              "          '153': 6,\n",
              "          '91': 5,\n",
              "          '71': 16,\n",
              "          '62': 19,\n",
              "          '79': 9,\n",
              "          '232': 1,\n",
              "          '151': 8,\n",
              "          '129': 7,\n",
              "          '202': 2,\n",
              "          '168': 3,\n",
              "          '15': 7,\n",
              "          '54': 30,\n",
              "          '162': 3,\n",
              "          '300': 1,\n",
              "          '102': 6,\n",
              "          '118': 5,\n",
              "          '221': 2,\n",
              "          '204': 1,\n",
              "          '114': 9,\n",
              "          '60': 13,\n",
              "          '248': 1,\n",
              "          '68': 12,\n",
              "          '150': 7,\n",
              "          '89': 12,\n",
              "          '104': 11,\n",
              "          '70': 13,\n",
              "          '17': 7,\n",
              "          '13': 6,\n",
              "          '158': 12,\n",
              "          '298': 1,\n",
              "          '125': 5,\n",
              "          '90': 5,\n",
              "          '18': 6,\n",
              "          '120': 5,\n",
              "          '16': 4,\n",
              "          '149': 9,\n",
              "          '117': 6,\n",
              "          '166': 6,\n",
              "          '136': 11,\n",
              "          '99': 8,\n",
              "          '177': 1,\n",
              "          '169': 3,\n",
              "          '101': 7,\n",
              "          '281': 2,\n",
              "          '138': 5,\n",
              "          '127': 8,\n",
              "          '103': 10,\n",
              "          '140': 7,\n",
              "          '87': 12,\n",
              "          '137': 4,\n",
              "          '123': 9,\n",
              "          '100': 4,\n",
              "          '133': 8,\n",
              "          '316': 2,\n",
              "          '226': 1,\n",
              "          '135': 6,\n",
              "          '112': 8,\n",
              "          '98': 2,\n",
              "          '215': 1,\n",
              "          '372': 1,\n",
              "          '231': 1,\n",
              "          '139': 5,\n",
              "          '276': 3,\n",
              "          '154': 11,\n",
              "          '106': 7,\n",
              "          '116': 6,\n",
              "          '220': 1,\n",
              "          '210': 1,\n",
              "          '12': 1,\n",
              "          '132': 6,\n",
              "          '93': 6,\n",
              "          '224': 1,\n",
              "          '128': 4,\n",
              "          '910': 1,\n",
              "          '188': 1,\n",
              "          '167': 2,\n",
              "          '164': 2,\n",
              "          '5': 8,\n",
              "          '327': 2,\n",
              "          '207': 1,\n",
              "          '203': 1,\n",
              "          '324': 1,\n",
              "          '172': 2,\n",
              "          '458': 1,\n",
              "          '212': 1,\n",
              "          '184': 1,\n",
              "          '611': 1,\n",
              "          '338': 1,\n",
              "          '11': 4,\n",
              "          '6': 3,\n",
              "          '205': 1,\n",
              "          '332': 1,\n",
              "          '191': 2,\n",
              "          '163': 4,\n",
              "          '790': 1,\n",
              "          '2': 1,\n",
              "          '431': 1,\n",
              "          '375': 1,\n",
              "          '363': 1,\n",
              "          '175': 2,\n",
              "          '113': 4,\n",
              "          '382': 1,\n",
              "          '588': 2,\n",
              "          '193': 1,\n",
              "          '446': 1,\n",
              "          '482': 1,\n",
              "          '444': 2,\n",
              "          '629': 1,\n",
              "          '235': 1,\n",
              "          '201': 1,\n",
              "          '245': 1,\n",
              "          '306': 1,\n",
              "          '243': 1,\n",
              "          '200': 1,\n",
              "          '10': 1,\n",
              "          '234': 1})}"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# these counters will keep the frequency of each word in ham and spam texts\n",
        "len_text_counters = {\n",
        "    'spam': Counter(), \n",
        "    'ham': Counter()\n",
        "}\n",
        "\n",
        "all_length = set()\n",
        "for label, text_len in zip(len_train_y, len_train_x):\n",
        "    length = str(text_len)\n",
        "    all_length.update([length])\n",
        "    # TODO: use the `update` methods of all 3 counters, to calculate total number of \n",
        "    len_text_counters[label].update([length])\n",
        "    \n",
        "# assert len_label_counter['spam'] == 409\n",
        "# assert len_text_counters['ham']['hello'] >= 2\n",
        "len_text_counters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "syP02W9jSGcw"
      },
      "outputs": [],
      "source": [
        "# this counter will keep the number of spam and ham texts\n",
        "label_counter = Counter()\n",
        "\n",
        "# these counters will keep the frequency of each word in ham and spam texts\n",
        "word_counters = {\n",
        "    'spam': Counter(), \n",
        "    'ham': Counter()\n",
        "}\n",
        "\n",
        "all_words = set()\n",
        "for label, words in zip(train_y, train_x):\n",
        "    all_words.update(words)\n",
        "    # TODO: use the `update` methods of all 3 counters, to calculate total number of \n",
        "    label_counter.update([label])\n",
        "    word_counters[label].update(words)\n",
        "    \n",
        "assert label_counter['spam'] == 409\n",
        "assert word_counters['ham']['hello'] >= 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32trpvYkSGc0"
      },
      "source": [
        "Now let's calculate different probabilities of words, texts, and labels for our classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "-58zZ0MdSGc1"
      },
      "outputs": [],
      "source": [
        "def prior_probability_of_label(label):\n",
        "    \"\"\" This function evaluates probability of the given label (it can be 'spam' or 'ham'), using the counters. \"\"\"\n",
        "    # TODO: calculate and return this probability as ratio of number of texts with this labels to number all texts\n",
        "    return label_counter[label] / (label_counter['ham'] + label_counter['spam'])\n",
        "\n",
        "assert round(prior_probability_of_label('spam'), 2) == 0.14\n",
        "assert round(prior_probability_of_label('ham'), 2) == 0.86"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.002"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def length_probability_given_label(length, label):\n",
        "    \"\"\" This function calculates probability of a word occurence in text, conditional on the label of this text. \"\"\"\n",
        "    # TODO: calculate and return this probability \n",
        "    # as ratio of number of texts with this word and label to number of texts with this label\n",
        "    alpha = 10**(-3)\n",
        "    p = 0.1\n",
        "    prob = (len_text_counters[label][length] + alpha*p) / (label_counter[label] + p)\n",
        "    if length in all_length:\n",
        "        return prob\n",
        "    return (1 - prob)\n",
        "\n",
        "round(length_probability_given_label(\"100\", \"spam\"), 3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "RB4bvSGjSGc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_probability_given_label(word, label):\n",
        "    \"\"\" This function calculates probability of a word occurence in text, conditional on the label of this text. \"\"\"\n",
        "    # TODO: calculate and return this probability \n",
        "    # as ratio of number of texts with this word and label to number of texts with this label\n",
        "    alpha = 10**(-3)\n",
        "    p = 0.1\n",
        "    return (word_counters[label][word] + alpha*p) / (label_counter[label] + p)\n",
        "\n",
        "assert round(word_probability_given_label(\"99\", \"spam\"), 3) == 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def word_probability_given_length(word, length):\n",
        "    \"\"\" This function calculates probability of a word occurence in text, conditional on the label of this text. \"\"\"\n",
        "    # TODO: calculate and return this probability \n",
        "    # as ratio of number of texts with this word and label to number of texts with this label\n",
        "    length = str(length)\n",
        "    alpha = 10**(-3)\n",
        "    p = 0.1\n",
        "    return (len_text_counters[label][word] + alpha*p) / (label_counter[label] + p)\n",
        "\n",
        "assert round(word_probability_given_label(\"99\", \"spam\"), 3) == 0.002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0MQSlyxSGdA"
      },
      "source": [
        "Here we encounter the first practical problem: some words have never occurred in our training data. \n",
        "\n",
        "But they can probably occur in the texts to which our model will be applied in the future. \n",
        "\n",
        "To assign a non-zero probability to such texts, we can slightly modify the `word_probability_given_label`. For example, instead of original estimate, \n",
        "\n",
        "$$\\hat{p}(word|label) = \\frac{count(word, label)}{count(label)}$$\n",
        "\n",
        "we could use a \"smoothed\" version\n",
        "\n",
        "$$\\hat{p}(word|label) = \\frac{count(word, label) + \\alpha\\times p}{count(label) + p}$$\n",
        "\n",
        "where $alpha\\in(0, 1)$ is the anchor probability towards which we move our estimate, and $p$ is the step size towards this anchor. \n",
        "\n",
        "Values like $p=0.1$ and $\\alpha=10^{-3}$ would do.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "63mpxYPlSGdC"
      },
      "outputs": [],
      "source": [
        "# TODO: modify the `word_probability_given_label` function, by moving each probability towards a small positive constant\n",
        "assert word_probability_given_label(\"999\", \"spam\") > 0\n",
        "assert word_probability_given_label(\"999\", \"spam\") < 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LeclJfNSGdG"
      },
      "source": [
        "Now we can move from words to texts. \n",
        "\n",
        "Here is where we apply our naive assumption that occurrences of each word are independent:\n",
        "$$ P(text|label) = \\prod_{word \\in text} P(word|label) \\times \\prod_{word \\notin text} (1-P(word|label)) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "iVZdBCwYSGdH"
      },
      "outputs": [],
      "source": [
        "def text_probability_given_label(text, label):\n",
        "    \"\"\" This function calculates probability of the text conditional on its label. \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = get_words(text)\n",
        "    probability = 1.0\n",
        "    # TODO: calculate the probability of text given label. \n",
        "    # use a function defined above and the naive assumption of word independence\n",
        "    for word in all_words:\n",
        "        if word in text:\n",
        "            probability = probability * word_probability_given_label(word,label)\n",
        "        else:\n",
        "            probability = probability * (1 - word_probability_given_label(word,label))\n",
        "    return probability\n",
        "\n",
        "greeting1 = 'hello how are you'\n",
        "greeting2 = 'hello teacher how are you'\n",
        "\n",
        "assert text_probability_given_label(greeting1, 'ham') > 0\n",
        "assert text_probability_given_label(greeting1, 'ham') < 0.0001\n",
        "assert text_probability_given_label(greeting2, 'ham') < text_probability_given_label(greeting1, 'ham')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQgZXCuDSGdM"
      },
      "source": [
        "Now you have all the components to compile your first probabilistic classifier!\n",
        "\n",
        "$$P(spam|text) = \\frac{P(spam)P(text|spam)}{P(spam)P(text|spam) + P(ham)P(text|ham)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "6OrHsTALSGdO"
      },
      "outputs": [],
      "source": [
        "def label_probability_given_text(text, label):\n",
        "    \"\"\" This function calculates probability of the label (spam or ham) conditional on the text. \"\"\"\n",
        "    # TODO: calculate label probability conditional on text\n",
        "    # use the Bayes rule and the functions defined above\n",
        "    comp_label = 'spam' if 'ham' == label else 'ham'\n",
        "    label_p = prior_probability_of_label(label)\n",
        "    text_given_label_p = text_probability_given_label(text,label)\n",
        "    label_complimentary_p = prior_probability_of_label(comp_label)\n",
        "    text_given_label_complimentary_p = text_probability_given_label(text,comp_label)\n",
        "    # something is missing, how do I calculate text probability?\n",
        "    return (label_p * text_given_label_p)/(label_p * text_given_label_p + label_complimentary_p * text_given_label_complimentary_p)\n",
        "\n",
        "\n",
        "text1 = 'hello how r you'\n",
        "text2 = 'only today you can buy our book with 50% discount!'\n",
        "\n",
        "assert label_probability_given_text(text1, 'ham') + label_probability_given_text(text1, 'spam') == 1.0\n",
        "assert label_probability_given_text(text1, 'ham') > label_probability_given_text(text1, 'spam')\n",
        "assert label_probability_given_text(text1, 'ham') > label_probability_given_text(text2, 'ham')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def label_probability_given_length(text, label):\n",
        "    \"\"\" This function calculates probability of the label (spam or ham) conditional on the text. \"\"\"\n",
        "    # TODO: calculate label probability conditional on text\n",
        "    # use the Bayes rule and the functions defined above\n",
        "    length = str(len(text))\n",
        "    comp_label = 'spam' if 'ham' == label else 'ham'\n",
        "    label_p = prior_probability_of_label(label)\n",
        "    len_p = length_probability_given_label(length,label)\n",
        "    label_c_p = prior_probability_of_label(comp_label)\n",
        "    len_c_p = length_probability_given_label(length,comp_label)\n",
        "    # something is missing, how do I calculate text probability?\n",
        "    return (label_p * len_p)/(label_p * len_p + label_c_p * len_c_p)\n",
        "\n",
        "\n",
        "text1 = 'hello how r you'\n",
        "text2 = 'only today you can buy our book with 50% discount!'\n",
        "\n",
        "label_probability_given_length(text1, 'ham') + label_probability_given_length(text1, 'spam') == 1.0\n",
        "label_probability_given_length(text1, 'ham') > label_probability_given_length(text1, 'spam')\n",
        "label_probability_given_length(text1, 'ham') > label_probability_given_length(text2, 'ham')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25DmEePrSGdR"
      },
      "source": [
        "## Tuning the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNJNZimSGdS"
      },
      "source": [
        "Now we have the classifier, but we don't know how well it works on the unseen data. \n",
        "\n",
        "Let's see what fraction of test messages are classified correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8685847589424572\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.5\n",
        "test_spam_probabilities = [label_probability_given_length(text, 'spam') for text in test_x]\n",
        "test_predictions = ['spam' if spamness > threshold else 'ham' for spamness in test_spam_probabilities]\n",
        "\n",
        "accuracy = sum(1 if pred == fact else 0 for pred, fact in zip(test_predictions, test_y)) / len(test_y)\n",
        "print(accuracy)\n",
        "\n",
        "# assert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1ZxEYeFGSGdT",
        "outputId": "953e492a-bf4f-442e-fdc8-7c5a277af61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9840590979782271\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.5\n",
        "test_spam_probabilities = [label_probability_given_text(text, 'spam') for text in test_x]\n",
        "test_predictions = ['spam' if spamness > threshold else 'ham' for spamness in test_spam_probabilities]\n",
        "\n",
        "accuracy = sum(1 if pred == fact else 0 for pred, fact in zip(test_predictions, test_y)) / len(test_y)\n",
        "print(accuracy)\n",
        "\n",
        "assert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating label probability given text AND length\n",
        "\n",
        "We want to calculate:\n",
        "$$P(spam|text∩length)$$\n",
        "\n",
        "I will assume that **P(text)** and **P(length)** are independent.\n",
        "\n",
        "$$P(spam|text∩length) = \\frac{P(spam)P(text|spam)P(length|spam)}{P(spam)P(text|spam)P(length|spam) + P(ham)P(text|ham)P(length|ham)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def label_probability_given_length_and_text(text, label):\n",
        "    \"\"\" This function calculates probability of the label (spam or ham) conditional on the text. \"\"\"\n",
        "    # TODO: calculate label probability conditional on text\n",
        "    # use the Bayes rule and the functions defined above\n",
        "    length = str(len(text))\n",
        "    comp_label = 'spam' if 'ham' == label else 'ham'\n",
        "    label_p = prior_probability_of_label(label)\n",
        "    len_p = length_probability_given_label(length,label)\n",
        "    label_c_p = prior_probability_of_label(comp_label)\n",
        "    len_c_p = length_probability_given_label(length,comp_label)\n",
        "\n",
        "    text_p = text_probability_given_label(text,label)\n",
        "    text_c_p = text_probability_given_label(text,comp_label)\n",
        "    # something is missing, how do I calculate text probability?\n",
        "    return (label_p * len_p * text_p)/((label_p * len_p * text_p) + (label_c_p * len_c_p * text_c_p))\n",
        "\n",
        "\n",
        "text1 = 'hello how r you'\n",
        "text2 = 'only today you can buy our book with 50% discount!'\n",
        "\n",
        "label_probability_given_length(text1, 'ham') + label_probability_given_length(text1, 'spam') == 1.0\n",
        "# label_probability_given_length(text1, 'ham') > label_probability_given_length(text1, 'spam')\n",
        "# label_probability_given_length(text1, 'ham') > label_probability_given_length(text2, 'ham')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9793934681181959\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.5\n",
        "test_spam_probabilities = [label_probability_given_length_and_text(text, 'spam') for text in test_x]\n",
        "test_predictions = ['spam' if spamness > threshold else 'ham' for spamness in test_spam_probabilities]\n",
        "\n",
        "accuracy = sum(1 if pred == fact else 0 for pred, fact in zip(test_predictions, test_y)) / len(test_y)\n",
        "print(accuracy)\n",
        "\n",
        "assert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying to lower case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qorvR9XvSGdo"
      },
      "source": [
        "This is a good accuracy, but you can achieve better results by tuning the algorithm. \n",
        "\n",
        "What you can do:\n",
        "* play with the different values of the threshold\n",
        "* play with the regularization constants that you used in `word_probability_given_label`\n",
        "* experiment with different implementations of `get_words` - e.g. ignore the word case, or use word lemmas\n",
        "* use your imagination\n",
        "\n",
        "Can you beat 99% accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGSStXnGSGd3"
      },
      "source": [
        "Have a good time! (-:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0eQuYMyZM7f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
