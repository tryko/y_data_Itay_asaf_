{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "train = pd.read_csv('./train.csv')\n",
    "test_ID = test.Id\n",
    "# test.drop(['Id'],axis=1,inplace=True)\n",
    "# train.drop(['Id'],axis=1,inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Simple Imputation of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_with_zero =['MasVnrArea','LotFrontage','GarageYrBlt','TotalBsmtSF','GarageCars','GarageArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','BsmtFullBath','BsmtHalfBath',]\n",
    "fill_with_none = ['Fence','MiscFeature','MasVnrType','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtQual','BsmtCond','PoolQC','Alley','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond']\n",
    "\n",
    "\n",
    "for col in fill_with_zero:\n",
    "    train[col].fillna(0,inplace=True)\n",
    "    test[col].fillna(0,inplace=True)\n",
    "\n",
    "for col in fill_with_none:\n",
    "    train[col].fillna(\"None\",inplace=True)\n",
    "    test[col].fillna(\"None\",inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Specific\n",
    "test = test.fillna({\"MSZoning\": \"RL\"})\n",
    "test = test.fillna({\"Exterior1st\": \"VinylSd\"})\n",
    "test = test.fillna({\"Exterior2nd\": \"VinylSd\"})\n",
    "train = train.fillna({\"Electrical\": \"SBrkr\"})\n",
    "test = test.fillna({\"KitchenQual\": \"TA\"})\n",
    "test = test.fillna({\"Functional\": \"Typ\"})\n",
    "test = test.fillna({\"SaleType\": \"WD\"})\n",
    "test.Utilities.fillna('AllPub',inplace=True)\n",
    "\n",
    "train = train.drop([\"Fence\", \"MiscFeature\", \"Utilities\"], axis=1)\n",
    "test = test.drop([\"Fence\", \"MiscFeature\", \"Utilities\"], axis=1)\n",
    "\n",
    "y = train[\"SalePrice\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Simple Handling of categorical with get_dummies And Aligning dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[: train.shape[0]]\n",
    "test_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[train.shape[0]:]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Creating Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 15, 'min_samples_split': 2, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 6, 8, 10],\n",
    "              'min_samples_split': [2, 4, 6, 8, 10],\n",
    "              'min_samples_leaf': [15,30,50],\n",
    "              'random_state': [42]}\n",
    "\n",
    "# Create the regressor\n",
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_dummies, y)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Get the best score\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters:\", best_params)\n",
    "baseline_model = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fit(train_dummies,y)\n",
    "pred = baseline_model.predict(test_dummies)\n",
    "output = pd.DataFrame({'Id': test_ID,\n",
    "                       'SalePrice': pred})\n",
    "\n",
    "output.to_csv('Baseline_submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Improving Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins> Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yosefos\\AppData\\Local\\Temp\\ipykernel_600\\2814956398.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  train.drop(train[(train.GrLivArea > 4000) & (y<300000)].index, inplace= True)\n"
     ]
    }
   ],
   "source": [
    "train.drop(train[train.LotFrontage> 200].index, inplace= True)\n",
    "train.drop(train[train.LotArea > 100000].index, inplace= True)\n",
    "train.drop(train[train.BsmtFinSF1 > 4000].index, inplace= True)\n",
    "train.drop(train[train.TotalBsmtSF > 6000].index, inplace= True)\n",
    "train.drop(train[train['1stFlrSF'] > 4000].index, inplace= True)\n",
    "train.drop(train[(train.GrLivArea > 4000) & (y<300000)].index, inplace= True)\n",
    "train.drop( train[ train.LowQualFinSF> 550].index, inplace= True)\n",
    "y = train.SalePrice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins> Converting Ordinal Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_cols = ['ExterCond','ExterQual', 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual']\n",
    "\n",
    "\n",
    "# mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0,\n",
    "#     'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1,\n",
    "#     'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1,\n",
    "#     'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0,\n",
    "#     'Fin': 3, 'RFn': 2, 'Unf': 1}\n",
    "\n",
    "\n",
    "# for col in ordinal_cols:\n",
    "#     train[col] = train[col].map(mapping)\n",
    "#     test[col] = test[col].map(mapping)\n",
    "\n",
    "# train.MSZoning = train.MSZoning.astype(str)\n",
    "# test.MSZoning = test.MSZoning.astype(str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins> Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_rooms = ['BedroomAbvGr','TotRmsAbvGrd']\n",
    "# n_bathrooms = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath','HalfBath']\n",
    "# porch_area = [ 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "\n",
    "# train['n_rooms'] = train[n_rooms].sum(axis=1)\n",
    "# train['n_bathrooms'] = train.BsmtFullBath + train.BsmtHalfBath * 0.5 + train.FullBath + train.HalfBath * 0.5\n",
    "# train['porch_area'] = train[porch_area].sum(axis=1)\n",
    "# train['years_till_remod'] = train.YearBuilt - train.YearRemodAdd \n",
    "# train['years_since_remod'] = train.YrSold - train.YearRemodAdd \n",
    "# train['age_at_sell'] = train.YrSold - train.YearRemodAdd\n",
    "# train['LowQualSF'] = train.LowQualFinSF + train.BsmtUnfSF\n",
    "\n",
    "\n",
    "# test['n_rooms'] = test[n_rooms].sum(axis=1)\n",
    "# test['n_bathrooms'] = test.BsmtFullBath + test.BsmtHalfBath * 0.5 + test.FullBath + test.HalfBath * 0.5\n",
    "# test['porch_area'] = test[porch_area].sum(axis=1)\n",
    "# test['years_till_remod'] = test.YearBuilt - test.YearRemodAdd \n",
    "# test['years_since_remod'] = test.YrSold - test.YearRemodAdd \n",
    "# test['age_at_sell'] = test.YrSold - test.YearBuilt\n",
    "# test['LowQualSF'] = test.LowQualFinSF + test.BsmtUnfSF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Convert y to Log to reduce skew for regression models and create dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(y)\n",
    "train_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[: train.shape[0]]\n",
    "test_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[train.shape[0]:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins> Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = train.select_dtypes(exclude='object').drop(['Id','SalePrice'],axis=1).columns\n",
    "\n",
    "# create StandardScaler object for train\n",
    "scaler = StandardScaler()\n",
    "train_dummies[cols_to_scale] = scaler.fit_transform(train_dummies[cols_to_scale])\n",
    "# scale the test columns using the scaler object from train\n",
    "test_dummies[cols_to_scale] = scaler.transform(test_dummies[cols_to_scale])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Moving From Basic Model to Ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Hypertuning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200, 'random_state': 42}\n",
      "Best Score:  0.013922279403980617\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,150,200],\n",
    "    'learning_rate': [0.05,0.01, 0.1],\n",
    "    'max_depth': [2,4, 5,10],\n",
    "    'random_state': [42]}\n",
    "best_GB = GradientBoostingRegressor()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "grid_search = GridSearchCV(best_GB, param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "grid_search.fit(train_dummies, y)\n",
    "best_GB = GradientBoostingRegressor(**grid_search.best_params_)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10.0, 'random_state': 42, 'solver': 'lsqr'}\n",
      "Best Score:  0.012903122225878546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.01,0.1, 1.0, 10.0],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "    'random_state': [42]}\n",
    "best_RR = Ridge()\n",
    "grid_search = GridSearchCV(best_RR, param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "grid_search.fit(train_dummies, y)\n",
    "best_RR = Ridge(**grid_search.best_params_)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Elastic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'l1_ratio': 0.05, 'random_state': 42}\n",
      "Best Score:  0.012631858699742126\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 0.01],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9, 1.0,0.1,0.05],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "best_EN = ElasticNet()\n",
    "\n",
    "grid_search = GridSearchCV(best_EN, param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(train_dummies, y)\n",
    "\n",
    "best_EN = ElasticNet(**grid_search.best_params_)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'max_features': 30, 'n_estimators': 150, 'random_state': 42}\n",
      "Best Score:  0.01782439590799286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100,150,200],\n",
    "    'max_depth': [2, 5, 10, 15,20],\n",
    "    'max_features': [10,20,30,'auto'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "best_RF = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(best_RF, param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(train_dummies, y)\n",
    "\n",
    "best_RF = RandomForestRegressor(**grid_search.best_params_)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0005, 'random_state': 42}\n",
      "Best Score:  0.012166685090905991\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{'alpha': [0.1,0.01,0.5,0.05,0.005,0.0005], 'random_state':[42]}]\n",
    "best_lasso = Lasso()\n",
    "grid_search = GridSearchCV(best_lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(train_dummies, y)\n",
    "best_lasso = Lasso(**grid_search.best_params_)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> Create best models (so i dont run gridsearch each time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_RF =  RandomForestRegressor(**{'max_depth': 15, 'max_features': 30, 'n_estimators': 200, 'random_state': 42})\n",
    "# best_EN = ElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.05, 'random_state': 42})\n",
    "# best_RR = Ridge(**{'alpha': 10.0, 'max_iter': 1000, 'random_state': 42, 'solver': 'sparse_cg'})\n",
    "# # best_GB = GradientBoostingRegressor(**{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200, 'random_state': 42})\n",
    "# best_lasso = Lasso(**{'alpha': 0.0005, 'random_state': 42})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Fit Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=15, max_features=30, n_estimators=150,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_EN.fit(train_dummies,y)\n",
    "best_GB.fit(train_dummies,y)\n",
    "best_lasso.fit(train_dummies,y)\n",
    "best_RR.fit(train_dummies,y)\n",
    "best_RF.fit(train_dummies,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_pred = np.exp(best_EN.predict(test_dummies))\n",
    "GB_pred = np.exp(best_GB.predict(test_dummies))\n",
    "RR_pred = np.exp(best_RR.predict(test_dummies))\n",
    "lasso_pred = np.exp(best_lasso.predict(test_dummies))\n",
    "RF_pred = np.exp(best_RF.predict(test_dummies))\n",
    "final_pred = (EN_pred +  + lasso_pred + +RR_pred+GB_pred)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test_ID,\n",
    "                       'SalePrice': final_pred})\n",
    "\n",
    "output.to_csv('all_models_normalized_OrdinalEncoding.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e2ac3774ab32f260f9760d004fae9073803f03c7ce8e55cd71c8539f29de38d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
