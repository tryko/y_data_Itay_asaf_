{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDmHTF5232la"
   },
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgpLAoVH32le"
   },
   "source": [
    "In this task you will practice dimensionality reduction.\n",
    "Use code cells to answer the Tasks and Markdown cells for the Questions (Q's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "7DLJ446A32lf"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuqIaglD32lg"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi1DNp5-32lh"
   },
   "outputs": [],
   "source": [
    "(X, y) = load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# split X into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB7dmNQP32lh"
   },
   "source": [
    "Lets take a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypYMfIuC32lh"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snVONeVb32li"
   },
   "source": [
    "# PCA + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dXgU9Dp32li"
   },
   "source": [
    "Task 1: Use X_train, y_train to train a SVM (SKlean's SVC) with the deafult parameters. You can read more about the algorithm in SKlearn's documentation.\n",
    "Make sure you normailize the data by using StandardScaler\n",
    "Evaulate the algorithm using accuracy score and X_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC()).fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCNEfuqy32lj"
   },
   "source": [
    "Task 2: Now do the same, but use PCA.\n",
    "\n",
    "In this task, we want to keep all of the variance! No data is going to be discarded.\n",
    "You are asked to use the maximal number of componenets for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCNEfuqy32lj"
   },
   "source": [
    "Q1: Your co-worker says that the results should be at least as good as the results we had without PCA. Explain why might he say that.\n",
    "    \n",
    "> PCA is just rotating the dataset so that the new coordinate system (or the principal components) would be ordered by the variance they capture. \n",
    ">\n",
    "> Using all PCs means we're just using the exact same data with a different coordinate system. Since we're giving our model the same data to work with, it makes sense we won't get different results (barring some regularization etc.)\n",
    "Print the accuracy of SVM + PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = make_pipeline(StandardScaler(), PCA(), SVC()).fit(X_train, y_train)\n",
    "y_pred_pca= pca_pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa-zokqL32lj"
   },
   "source": [
    "Q2: Did the results improve\\stayed the same\\got worse? \n",
    "    \n",
    "> Got the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPdbsJQh32lj"
   },
   "source": [
    "# PCA + logistice regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7ZOUCLC32lk"
   },
   "source": [
    "Task 3: repeat task 1 with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression()).fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: repeast task 2 with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_pipe = make_pipeline(StandardScaler(), PCA(), LogisticRegression()).fit(X_train, y_train)\n",
    "y_pred_pca = pca_pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP1L5aqw32lk"
   },
   "source": [
    "Q3: Did the results improved\\stayed the same\\got worse?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En4ptRLe32ll"
   },
   "source": [
    "Q4: How can you explain the difference between answers to Q2 and Q3. Hint: think about the nature of Logistic regression and the main difference of SVM from it. Hint: SVM assumes the data can be seperated by an hyperplan.\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_PfC__h32ll"
   },
   "source": [
    "# Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMpkIEQ632ll"
   },
   "source": [
    "Task 5: Use locally linear embedding in sklearn to visualize the data. Plot the results.\n",
    "Optimze the n_neighbors by running at least 5 times and use the best looking result you can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import locally_linear_embedding\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ns = range(45, 146, 25)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, sharex=True, sharey=True)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(4)\n",
    "\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for ax, n in zip(axs.flatten(), ns):\n",
    "    if n==95:\n",
    "        for side in ['bottom', 'top', 'right', 'left']:\n",
    "            ax.spines[side].set_color('0.2')\n",
    "            ax.spines[side].set_linewidth(4)\n",
    "            ax.set_title(f'This one seems ok (n={n})')\n",
    "    ax.set_box_aspect(1)\n",
    "    X_emb = locally_linear_embedding(X_scaled, n_neighbors=n, n_components=2)[0]\n",
    "    sns.scatterplot(X_emb x=[:,0], y=X_emb[:,1], hue=y, ax=ax, legend=0)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHC6gNno32ll"
   },
   "source": [
    "Task 6: Use t-SNE to visualize the data. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yellowbrick.features.manifold import manifold_embedding\n",
    "manifold_embedding(X_scaled, y, normalized_stress='auto', manifold='tsne');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PTfZokr32ll"
   },
   "source": [
    "Task 7: Use UMAP to visualize the data. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "X_emb = umap.UMAP().fit_transform(X_scaled)\n",
    "sns.scatterplot(x=X_emb[:,0], y=X_emb[:,1], hue=y, palette='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z_GjviC32lm"
   },
   "source": [
    "Q5: If we run one of this visualziaing algorithms various times with the default parameters, are we guranteed to see the same results? Why?\n",
    "    \n",
    "> LLE is deterministic and would yield the same results. \n",
    ">\n",
    "> UMAP and t-SNE would not, since they work through stochastic gradient descent\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "97bf5cac092693491f9f0c093044444828bd281c57f8a7f0d4a6df9d6c416292"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
