{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Dask and why we need tools like it\n",
    "\n",
    "There are no problems with processing datasets of up to several Gb, be it some computational task or machine learning model training. Single, although powerful enough, machine can handle such volume easily.\n",
    "\n",
    "It's a bit more elaborated to process tens of `Gb` or more, or **speed-up training** of complex models. Since vertical scaling is always limited by how large the machine is, there's usually no other way, but to go for horizontal scaling and some type of parallelism.\n",
    "\n",
    "**Dask** offers tools for this exact case. For example,\n",
    "\n",
    "- you may want to **leverage all the cores** of your current machine to speed-up the computations, but do not want to to for `multiprocessing`,\n",
    "- alternatively, you may need to **process data too large** for machine's memory, which is called **out-of-core processing**,\n",
    "- or you may need a **unified setup** for both local parallelism (for prototyping) and distributed cloud-based computation.\n",
    "\n",
    "Many interesting problems in machine learning are simply not solvable on a single machine and Dask offers a great and simple way to introduce parallelism into your problem.\n",
    "\n",
    "Another benefit is that Dask is written in Python, so there's no need to use tricky to set up Scala-based Spark.\n",
    "\n",
    "In this tutorial we will use **local** setup, i.e. Dask cluster will run on a single machine. The main benefit is full-utilization of all machine cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T08:02:40.491455Z",
     "start_time": "2020-01-04T08:02:40.487111Z"
    }
   },
   "source": [
    "# Dask cluster\n",
    "\n",
    "First, we need to create a Dask cluster and a Dask client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:02.133994Z",
     "start_time": "2020-01-11T15:02:01.069921Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:04.777671Z",
     "start_time": "2020-01-11T15:02:03.399063Z"
    }
   },
   "outputs": [],
   "source": [
    "# you may want to change `n_workers` according to your hardware setup\n",
    "cluster = LocalCluster(n_workers=12)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:06.464505Z",
     "start_time": "2020-01-11T15:02:06.275263Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a **cluster** of 12 nodes, and connected to it as a client. Note that you can directly control cluster size from the notebook.\n",
    "\n",
    "Under the hood, Dask cluster contains **scheduler**, which is responsible for handling computations and spreading them between nodes. Scheduler can be launched also from the command line (see [Command Line](https://docs.dask.org/en/latest/setup/cli.html) section of documentation).\n",
    "\n",
    "Dask also provides nice realtime **dashboard** to overview tasks and workers (see link in the cell output above).\n",
    "\n",
    "We can now submit tasks to Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:06:44.198861Z",
     "start_time": "2020-01-11T16:06:44.178729Z"
    }
   },
   "outputs": [],
   "source": [
    "result_future = client.submit(np.sin, np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `client.submit` creates what is called **future**, i.e. a handle to task result, which is available as soon as computation completes.\n",
    "\n",
    "You can retrieve task status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:07:47.244817Z",
     "start_time": "2020-01-11T16:07:47.235658Z"
    }
   },
   "outputs": [],
   "source": [
    "result_future.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:08:49.208724Z",
     "start_time": "2020-01-11T16:08:49.186407Z"
    }
   },
   "outputs": [],
   "source": [
    "result_future.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:09:17.606687Z",
     "start_time": "2020-01-11T16:09:17.581838Z"
    }
   },
   "outputs": [],
   "source": [
    "result_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:10:11.160658Z",
     "start_time": "2020-01-11T16:10:11.139869Z"
    }
   },
   "source": [
    "You can also submit **multiple tasks** at once (we recommend to open Dask dashboard alongside and observe how tasks start and proceed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:13:46.861073Z",
     "start_time": "2020-01-11T16:13:46.701193Z"
    }
   },
   "outputs": [],
   "source": [
    "futures = [client.submit(np.sin, x) for x in np.random.randn(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the results, we need to **gather** them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:14:26.716447Z",
     "start_time": "2020-01-11T16:14:26.636367Z"
    }
   },
   "outputs": [],
   "source": [
    "results = client.gather(futures)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:14:56.658037Z",
     "start_time": "2020-01-11T16:14:56.645565Z"
    }
   },
   "source": [
    "Dask also allows for straightforward **chaining** of tasks (note that `s`, `s_sq` and `s_full` are all futures, not Numpy arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:26:28.250361Z",
     "start_time": "2020-01-11T16:26:28.206786Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10000)\n",
    "s = client.submit(np.sin, x)\n",
    "s_sq = client.submit(np.square, x)\n",
    "s_full = client.submit(np.add, s, s_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:26:46.158275Z",
     "start_time": "2020-01-11T16:26:46.137996Z"
    }
   },
   "outputs": [],
   "source": [
    "s_full.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and ML\n",
    "\n",
    "As a specific and relevant example of parallelization for machine learning, we will consider parallel grid search. Imagine, that you need to fit a parametrized machine learning model (almost all ML models have some parameters).\n",
    "\n",
    "To find a good set of hyperparameters, you need to fit a model set of parameters. The main Python package for classical machine learning - `scikit-learn` or `sklearn` for short - allows you to do that easily. We will use the Titanic dataset and create a simple classification model for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to change the location according to your local setup\n",
    "DATA_DIR = pathlib.Path(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR.joinpath(\"train.csv\"), index_col=\"PassengerId\")\n",
    "test = pd.read_csv(DATA_DIR.joinpath(\"test.csv\"), index_col=\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_imputation = train.groupby([\"Pclass\", \"Sex\"])[\"Age\"].mean()\n",
    "\n",
    "train = train.join(age_imputation,\n",
    "                   on=(\"Pclass\", \"Sex\"),\n",
    "                   rsuffix=\"_imp\")\n",
    "\n",
    "train.loc[train.Age.isnull(), \"Age\"] = train.loc[train.Age.isnull(), \"Age_imp\"]\n",
    "train.drop(\"Age_imp\", axis=1, inplace=True)\n",
    "\n",
    "test = test.join(age_imputation,\n",
    "                 on=(\"Pclass\", \"Sex\"),\n",
    "                 rsuffix=\"_imp\")\n",
    "\n",
    "test.loc[test.Age.isnull(), \"Age\"] = test.loc[test.Age.isnull(), \"Age_imp\"]\n",
    "test.drop(\"Age_imp\", axis=1, inplace=True)\n",
    "\n",
    "most_frequent_port = train.Embarked.value_counts().idxmax()\n",
    "average_fare = train.Fare.mean()\n",
    "\n",
    "train.fillna({\"Embarked\": most_frequent_port}, inplace=True)\n",
    "test.fillna({\"Embarked\": most_frequent_port, \"Fare\": average_fare}, inplace=True)\n",
    "\n",
    "train.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n",
    "test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "test = pd.get_dummies(test, columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "\n",
    "FEATURES_COLS = train.columns[1:]\n",
    "TARGET = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our dataset contains no missing values, is fully numeric, so that we can start modeling it. We will use random forest model. You do not need to understand it in full right now, but the main idea is to combine a lot of weak estimators (decision trees) and get a better result overall.\n",
    "\n",
    "We will also use cross-validation, since it's a crucial part of hyperparameters search. The main idea is, again, simple: you train your models on a part of a dataset, you choose model parameters based on model performance on a different part (previously unseen to reduce overfitting risk, i.e. you cross-validate your model), and then you assess the final model performance with the best parameters on a test set. i.e. test your final model.\n",
    "\n",
    "`sklearn` provides convenient classes for the entire grid search process. We will use 4-fold cross-validation: for each set of parameters, training dataset will be split in 4 equal parts, and four models will be fitted with that set of parameters in such a way that each model is cross-validated on one of four fold, and the remaining 3 are used for training.\n",
    "\n",
    "`joblib` is the job manager, which dispatches calculations under the hood and can use different backends to do that in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specify parameters grid. During grid search, all combinations will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": [2,4,6],\n",
    "    \"n_estimators\": [100, 200, 500],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model instance, so that `sklearn` knows which model we want. In `sklearn` API model parameters can be directly set, hence, we create an \"empty\" model, which will serve as a blueprint.\n",
    "\n",
    "Also note, that we do not provide the scoring criterion. By default, `GridSearchCV` will use whatever scoring the model uses. In the case of `RandomForestClassifier` it's accuracy, which is exactly the metrics Kaggle uses for this dataset.\n",
    "\n",
    "We now can launch the grid search itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "grid_cv = GridSearchCV(model, params, cv=4, verbose=1)\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    grid_cv.fit(train[FEATURES_COLS], train[TARGET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib` will use the local cluster we created to distribute the training jobs and run them in parallel. The best parameters for the features we have are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score, correspondingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grid search finds the best parameters, by default it refits the model on the entire training set, so that we do not need to do that manually. Effectively, we now have a random forest model, trained on the entire training set with the best model parameters. Let's use it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(grid_cv.best_estimator_.predict(test[FEATURES_COLS]),\n",
    "                          index=test.index, columns=[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_DIR.joinpath(\"dask_submission.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions get about `0.775` when submitted to Kaggle.\n",
    "\n",
    "\n",
    "# Final remarks\n",
    "\n",
    "You may consider this an overkill for this specific model. That is true, and anyway `joblib` can handle local parallelism well enough. However, imagine that you're searching over a **huge grid** and have a **standby Dask cluster in the cloud**: in that case this setup will serve its purpose really well.\n",
    "\n",
    "We haven't covered a lot of technical details about Dask (resource quoting, deployment and others), as well as out-of-core processing, but hopefully you got a feeling of it and will dig further as soon as you'll encounter long-running grid/random search or alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
